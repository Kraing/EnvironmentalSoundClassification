{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "congressional-decline",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "\n",
    "import Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "becoming-villa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved segments\n",
    "def Load_Segments(dataset, fold):\n",
    "    \n",
    "    if dataset=='ESC10':\n",
    "        if fold==1:\n",
    "            hf = h5py.File('ESC10/MF1.h5', 'r')    \n",
    "        if fold==2:\n",
    "            hf = h5py.File('ESC10/MF2.h5', 'r')\n",
    "        if fold==3:\n",
    "            hf = h5py.File('ESC10/MF3.h5', 'r')\n",
    "        if fold==4:\n",
    "            hf = h5py.File('ESC10/MF4.h5', 'r')\n",
    "        if fold==5:\n",
    "            hf = h5py.File('ESC10/MF5.h5', 'r')\n",
    "\n",
    "    if dataset=='ESC50':\n",
    "        if fold==1:\n",
    "            hf = h5py.File('ESC50/MF1.h5', 'r')    \n",
    "        if fold==2:\n",
    "            hf = h5py.File('ESC50/MF2.h5', 'r')\n",
    "        if fold==3:\n",
    "            hf = h5py.File('ESC50/MF3.h5', 'r')\n",
    "        if fold==4:\n",
    "            hf = h5py.File('ESC50/MF4.h5', 'r')\n",
    "        if fold==5:\n",
    "            hf = h5py.File('ESC50/MF5.h5', 'r')\n",
    "    \n",
    "    # Get training\n",
    "    train_d = np.array(hf.get('train_data'))\n",
    "    train_l = np.array(hf.get('train_label'))\n",
    "\n",
    "\n",
    "    # Get validation\n",
    "    val_d = np.array(hf.get('validation_data'))\n",
    "    val_l = np.array(hf.get('validation_label'))\n",
    "\n",
    "    # Get test\n",
    "    test_d = np.array(hf.get('test_label'))\n",
    "    test_l = np.array(hf.get('test_label'))\n",
    "\n",
    "    hf.close()\n",
    "    \n",
    "    # Cast to float32\n",
    "    train_d = np.asarray(train_d, dtype=np.float32)\n",
    "    train_l = np.asarray(train_l, dtype=np.float32)\n",
    "    \n",
    "    val_d = np.asarray(val_d, dtype=np.float32)\n",
    "    val_l = np.asarray(val_l, dtype=np.float32)\n",
    "    \n",
    "    test_d = np.asarray(test_d, dtype=np.float32)\n",
    "    test_l = np.asarray(test_l, dtype=np.float32)\n",
    "    \n",
    "    return train_d, train_l, val_d, val_l, test_d, test_l\n",
    "\n",
    "\n",
    "def CreateTrainingSet10(data, label, batch_size=32):\n",
    "    \n",
    "    # Shuffle the folds\n",
    "    rnd_indices = np.arange(0, len(data))\n",
    "    rnd_indices = np.random.shuffle(rnd_indices)\n",
    "    \n",
    "    data = data[rnd_indices].reshape((len(data), 60, 41, 3))\n",
    "    label = label[rnd_indices].reshape((len(label), 10))\n",
    "    \n",
    "    \n",
    "    data = data.astype(np.float32)\n",
    "    label = label.astype(np.float32)\n",
    "    \n",
    "    '''\n",
    "    datagen = tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range=0,\n",
    "                                                              height_shift_range=0,\n",
    "                                                              horizontal_flip=False,\n",
    "                                                              vertical_flip=False) \n",
    "    \n",
    "    # Shuffle all elements at every iteration\n",
    "    training_dataset = datagen.flow(data, label, batch_size=batch_size, shuffle=True)\n",
    "    '''\n",
    "    \n",
    "    training_dataset = tf.data.Dataset.from_tensor_slices((data, label))\n",
    "    \n",
    "    # Shuffle all elements at every iteration\n",
    "    training_dataset = training_dataset.shuffle(len(training_dataset))\n",
    "    \n",
    "    # Define batch_size and prefetch size\n",
    "    training_dataset = training_dataset.batch(batch_size=batch_size).prefetch(buffer_size=1)\n",
    "    \n",
    "    return training_dataset\n",
    "\n",
    "\n",
    "def CreateTrainingSet50(data, label, batch_size=32):\n",
    "    \n",
    "    # Shuffle the folds\n",
    "    rnd_indices = np.arange(0, len(data))\n",
    "    rnd_indices = np.random.shuffle(rnd_indices)\n",
    "    \n",
    "    data = data[rnd_indices].reshape((len(data), 60, 41, 3))\n",
    "    label = label[rnd_indices].reshape((len(label), 50))\n",
    "    \n",
    "    \n",
    "    data = data.astype(np.float32)\n",
    "    label = label.astype(np.float32)\n",
    "    \n",
    "    '''\n",
    "    datagen = tf.keras.preprocessing.image.ImageDataGenerator(width_shift_range=0,\n",
    "                                                              height_shift_range=0,\n",
    "                                                              horizontal_flip=False,\n",
    "                                                              vertical_flip=False) \n",
    "    \n",
    "    # Shuffle all elements at every iteration\n",
    "    training_dataset = datagen.flow(data, label, batch_size=batch_size, shuffle=True)\n",
    "    '''\n",
    "    \n",
    "    training_dataset = tf.data.Dataset.from_tensor_slices((data, label))\n",
    "    \n",
    "    # Shuffle all elements at every iteration\n",
    "    training_dataset = training_dataset.shuffle(len(training_dataset))\n",
    "    \n",
    "    # Define batch_size and prefetch size\n",
    "    training_dataset = training_dataset.batch(batch_size=batch_size).prefetch(buffer_size=1)\n",
    "    \n",
    "    return training_dataset\n",
    "\n",
    "\n",
    "def CreateValidationSet(data, label, batch_size=32):\n",
    "    \n",
    "    data = data.astype(dtype=np.float32)\n",
    "    label = label.astype(dtype=np.float32)\n",
    "    \n",
    "    # Create and cache training\n",
    "    validation_dataset = tf.data.Dataset.from_tensor_slices((data, label))\n",
    "    \n",
    "    # Cache dataset\n",
    "    #validation_dataset = validation_dataset.cache(name)\n",
    "    \n",
    "    # Define batch_size and prefetch size\n",
    "    validation_dataset = validation_dataset.batch(batch_size=batch_size).prefetch(buffer_size=1)\n",
    "    \n",
    "    return validation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "yellow-trial",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "max_epochs=50\n",
    "\n",
    "train_d, train_l, val_d, val_l, test_d, test_l = Load_Segments('ESC10', 1)\n",
    "\n",
    "training_dataset = CreateTrainingSet10(train_d, train_l, batch_size=batch_size)\n",
    "validation_dataset = CreateValidationSet(val_d, val_l, batch_size=batch_size)\n",
    "    \n",
    "# Initialize the network\n",
    "net = Networks.PiczakNet10([60, 41, 3])\n",
    "loss_f = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "opt = tf.keras.optimizers.Adam(lr=0.002)\n",
    "net.compile(optimizer=opt, loss=loss_f, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "written-gardening",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1: \t t-loss: 15.022686 \t t-acc: 0.108429 \t v-loss: 8.366318 \t v-acc: 0.112500 \t time: 10.195\n",
      "Epoch  2: \t t-loss: 6.807284 \t t-acc: 0.104230 \t v-loss: 5.651976 \t v-acc: 0.104688 \t time: 7.138\n",
      "Epoch  3: \t t-loss: 5.032977 \t t-acc: 0.102617 \t v-loss: 4.531196 \t v-acc: 0.109375 \t time: 7.165\n",
      "Epoch  4: \t t-loss: 4.221077 \t t-acc: 0.101886 \t v-loss: 3.953943 \t v-acc: 0.112500 \t time: 7.091\n",
      "Epoch  5: \t t-loss: 3.769847 \t t-acc: 0.115053 \t v-loss: 3.606511 \t v-acc: 0.112500 \t time: 7.089\n",
      "Epoch  6: \t t-loss: 3.485084 \t t-acc: 0.103855 \t v-loss: 3.372072 \t v-acc: 0.112500 \t time: 7.217\n",
      "Epoch  7: \t t-loss: 3.283621 \t t-acc: 0.107273 \t v-loss: 3.199760 \t v-acc: 0.112500 \t time: 7.174\n",
      "Epoch  8: \t t-loss: 3.129577 \t t-acc: 0.105647 \t v-loss: 3.064675 \t v-acc: 0.112500 \t time: 7.141\n",
      "Epoch  9: \t t-loss: 3.008466 \t t-acc: 0.104637 \t v-loss: 2.955179 \t v-acc: 0.112500 \t time: 7.155\n",
      "Epoch 10: \t t-loss: 2.909185 \t t-acc: 0.106314 \t v-loss: 2.865012 \t v-acc: 0.112500 \t time: 7.302\n",
      "Epoch 11: \t t-loss: 2.826416 \t t-acc: 0.109261 \t v-loss: 2.788839 \t v-acc: 0.111719 \t time: 7.371\n",
      "Epoch 12: \t t-loss: 2.755394 \t t-acc: 0.102928 \t v-loss: 2.723978 \t v-acc: 0.112500 \t time: 7.219\n",
      "Epoch 13: \t t-loss: 2.695832 \t t-acc: 0.106199 \t v-loss: 2.668951 \t v-acc: 0.112500 \t time: 7.244\n",
      "Epoch 14: \t t-loss: 2.644210 \t t-acc: 0.103773 \t v-loss: 2.620993 \t v-acc: 0.112500 \t time: 7.209\n",
      "Epoch 15: \t t-loss: 2.599924 \t t-acc: 0.104198 \t v-loss: 2.580069 \t v-acc: 0.112500 \t time: 7.176\n",
      "Epoch 16: \t t-loss: 2.561262 \t t-acc: 0.113688 \t v-loss: 2.544402 \t v-acc: 0.112500 \t time: 7.241\n",
      "Epoch 17: \t t-loss: 2.528041 \t t-acc: 0.116438 \t v-loss: 2.513026 \t v-acc: 0.112500 \t time: 7.283\n",
      "Epoch 18: \t t-loss: 2.498938 \t t-acc: 0.109813 \t v-loss: 2.486018 \t v-acc: 0.112500 \t time: 7.327\n",
      "Epoch 19: \t t-loss: 2.473763 \t t-acc: 0.108543 \t v-loss: 2.462211 \t v-acc: 0.112500 \t time: 7.337\n",
      "Epoch 20: \t t-loss: 2.451723 \t t-acc: 0.110105 \t v-loss: 2.442209 \t v-acc: 0.112500 \t time: 7.248\n",
      "Epoch 21: \t t-loss: 2.432474 \t t-acc: 0.100210 \t v-loss: 2.424513 \t v-acc: 0.111719 \t time: 7.310\n",
      "Epoch 22: \t t-loss: 2.416281 \t t-acc: 0.108200 \t v-loss: 2.408600 \t v-acc: 0.112500 \t time: 7.341\n",
      "Epoch 23: \t t-loss: 2.401638 \t t-acc: 0.107762 \t v-loss: 2.394915 \t v-acc: 0.112500 \t time: 7.253\n",
      "Epoch 24: \t t-loss: 2.388390 \t t-acc: 0.112595 \t v-loss: 2.383355 \t v-acc: 0.109375 \t time: 7.226\n",
      "Epoch 25: \t t-loss: 2.377464 \t t-acc: 0.100908 \t v-loss: 2.372797 \t v-acc: 0.112500 \t time: 7.176\n",
      "Epoch 26: \t t-loss: 2.367926 \t t-acc: 0.109356 \t v-loss: 2.363721 \t v-acc: 0.111719 \t time: 7.227\n",
      "Epoch 27: \t t-loss: 2.359187 \t t-acc: 0.102293 \t v-loss: 2.355873 \t v-acc: 0.112500 \t time: 7.145\n",
      "Epoch 28: \t t-loss: 2.352005 \t t-acc: 0.100438 \t v-loss: 2.348865 \t v-acc: 0.112500 \t time: 7.098\n",
      "Epoch 29: \t t-loss: 2.345624 \t t-acc: 0.113783 \t v-loss: 2.342633 \t v-acc: 0.112500 \t time: 7.101\n",
      "Epoch 30: \t t-loss: 2.339395 \t t-acc: 0.103595 \t v-loss: 2.337422 \t v-acc: 0.112500 \t time: 7.112\n",
      "Epoch 31: \t t-loss: 2.334380 \t t-acc: 0.110626 \t v-loss: 2.332921 \t v-acc: 0.111719 \t time: 7.103\n",
      "Epoch 32: \t t-loss: 2.329880 \t t-acc: 0.107209 \t v-loss: 2.328723 \t v-acc: 0.112500 \t time: 7.244\n",
      "Epoch 33: \t t-loss: 2.326224 \t t-acc: 0.102001 \t v-loss: 2.325189 \t v-acc: 0.112500 \t time: 7.163\n",
      "Epoch 34: \t t-loss: 2.322517 \t t-acc: 0.101709 \t v-loss: 2.321820 \t v-acc: 0.112500 \t time: 7.105\n",
      "Epoch 35: \t t-loss: 2.319550 \t t-acc: 0.116648 \t v-loss: 2.319132 \t v-acc: 0.111719 \t time: 7.100\n",
      "Epoch 36: \t t-loss: 2.316983 \t t-acc: 0.114564 \t v-loss: 2.316713 \t v-acc: 0.112500 \t time: 7.163\n",
      "Epoch 37: \t t-loss: 2.314601 \t t-acc: 0.107095 \t v-loss: 2.314349 \t v-acc: 0.112500 \t time: 7.227\n",
      "Epoch 38: \t t-loss: 2.312435 \t t-acc: 0.110741 \t v-loss: 2.312448 \t v-acc: 0.112500 \t time: 7.190\n",
      "Epoch 39: \t t-loss: 2.309731 \t t-acc: 0.119627 \t v-loss: 2.310720 \t v-acc: 0.112500 \t time: 7.182\n",
      "Epoch 40: \t t-loss: 2.309551 \t t-acc: 0.098761 \t v-loss: 2.308803 \t v-acc: 0.112500 \t time: 7.203\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-00db37e8fdcd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;31m# train on batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mstep_stats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;31m# save loss and accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_gpu_hda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[0;32m   1696\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1697\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1698\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1699\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1700\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_gpu_hda\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mreset_metrics\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1635\u001b[0m     \"\"\"\n\u001b[0;32m   1636\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1637\u001b[1;33m       \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1638\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1639\u001b[0m   def train_on_batch(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_gpu_hda\\lib\\site-packages\\tensorflow\\python\\keras\\metrics.py\u001b[0m in \u001b[0;36mreset_states\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    245\u001b[0m     \u001b[0mwhen\u001b[0m \u001b[0ma\u001b[0m \u001b[0mmetric\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mevaluated\u001b[0m \u001b[0mduring\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    246\u001b[0m     \"\"\"\n\u001b[1;32m--> 247\u001b[1;33m     \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    248\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mabc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_gpu_hda\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_gpu_hda\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36mbatch_set_value\u001b[1;34m(tuples)\u001b[0m\n\u001b[0;32m   3574\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly_outside_functions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3575\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtuples\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3576\u001b[1;33m       \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3577\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3578\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_gpu_hda\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36massign\u001b[1;34m(self, value, use_locking, name, read_value)\u001b[0m\n\u001b[0;32m    858\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_is_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue_tensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    859\u001b[0m       assign_op = gen_resource_variable_ops.assign_variable_op(\n\u001b[1;32m--> 860\u001b[1;33m           self.handle, value_tensor, name=name)\n\u001b[0m\u001b[0;32m    861\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mread_value\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    862\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lazy_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0massign_op\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf_gpu_hda\\lib\\site-packages\\tensorflow\\python\\ops\\gen_resource_variable_ops.py\u001b[0m in \u001b[0;36massign_variable_op\u001b[1;34m(resource, value, name)\u001b[0m\n\u001b[0;32m    142\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0;32m    143\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"AssignVariableOp\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m         tld.op_callbacks, resource, value)\n\u001b[0m\u001b[0;32m    145\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch_loss= []\n",
    "epoch_acc = []\n",
    "\n",
    "epoch_vl = []\n",
    "epoch_va = []\n",
    "\n",
    "# Loop over the epochs\n",
    "for epoch in range(max_epochs):\n",
    "\n",
    "\n",
    "    step_loss = []\n",
    "    step_acc = []\n",
    "\n",
    "    step_vl = []\n",
    "    step_va = []\n",
    "\n",
    "    batches = 0\n",
    "    start = time.time()\n",
    "    # train over mini-batches\n",
    "    for x_batch, y_batch in training_dataset:\n",
    "\n",
    "        # train on batch\n",
    "        step_stats = net.train_on_batch(x_batch, y_batch)\n",
    "\n",
    "        # save loss and accuracy\n",
    "        step_loss.append(step_stats[0])\n",
    "        step_acc.append(step_stats[1])\n",
    "\n",
    "\n",
    "    # compute validation stats\n",
    "    for x_batch, y_batch in validation_dataset:\n",
    "\n",
    "        # compute validation stats\n",
    "        val_stats = net.test_on_batch(x_batch, y_batch)\n",
    "\n",
    "        # save loss and accuracy\n",
    "        step_vl.append(val_stats[0])\n",
    "        step_va.append(val_stats[1])\n",
    "\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    # Save the mean loss and accuracy of the entire epoch\n",
    "    epoch_loss.append(np.mean(step_loss))\n",
    "    epoch_acc.append(np.mean(step_acc))\n",
    "    epoch_vl.append(np.mean(step_vl))\n",
    "    epoch_va.append(np.mean(step_va))\n",
    "\n",
    "    # Print epoch training stats\n",
    "    print(\"Epoch %2d: \\t t-loss: %3.6f \\t t-acc: %.6f \\t v-loss: %3.6f \\t v-acc: %.6f \\t time: %3.3f\" % (epoch + 1, epoch_loss[-1], epoch_acc[-1], epoch_vl[-1], epoch_va[-1], (end - start)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heated-brand",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(epoch_loss, label='train loss')\n",
    "plt.plot(epoch_vl, label='train acc')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.title('Loss Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "starting-statement",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(epoch_acc, label='train acc')\n",
    "plt.plot(epoch_va, label='val acc')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.ylim(0, 1)\n",
    "plt.title('Accuracy Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funky-import",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.save('saved_model/P10_F1_Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "familiar-yacht",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
