{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "important-puzzle",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "\n",
    "from glob import glob\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import h5py\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display\n",
    "import librosa.display\n",
    "\n",
    "# Use GPU\n",
    "from tensorflow.python.client import device_lib \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "featured-banks",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "historical-technician",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Load_ESC10(path):\n",
    "    '''\n",
    "        Input:\n",
    "            path: folder of the dataset\n",
    "        \n",
    "        Output:\n",
    "            raw_data:  list that contains the raw data\n",
    "            cvs:       list that contains the cross-fold number\n",
    "            labels:    list that contains the category information\n",
    "    '''\n",
    "    \n",
    "    # Container for the dataset\n",
    "    raw_data = []\n",
    "    cvs = []\n",
    "    labels = []\n",
    "\n",
    "    # Extract ESC10 files name\n",
    "    df = pd.read_csv(glob('meta/esc50.csv')[0])\n",
    "\n",
    "    # filter columns\n",
    "    df = df[['filename', 'esc10']]\n",
    "\n",
    "    # Load every file inside the folder\n",
    "    for file_name in tqdm(os.listdir(path)):\n",
    "\n",
    "        # Check if file_name is an esc10\n",
    "        row = df[df['filename']==file_name]\n",
    "        check = row.esc10.iloc[0]\n",
    "\n",
    "        if check==True:    \n",
    "            try:\n",
    "                # Get audio data and sampling rate\n",
    "                audio, sampling_rate = librosa.load(os.path.join(path, file_name), res_type='kaiser_fast')\n",
    "\n",
    "                # Split the file name\n",
    "                name_splitted = re.split('[-.]', file_name)\n",
    "\n",
    "                # Append a row of 3 elements\n",
    "                raw_data.append(audio)\n",
    "                cvs.append(name_splitted[0])\n",
    "                labels.append(name_splitted[3])\n",
    "                                \n",
    "            except Exception as e:\n",
    "                pass\n",
    "    raw_audio = np.asarray(raw_data)\n",
    "    cvs = np.asarray(cvs, dtype=int)\n",
    "    labels = np.asarray(labels, dtype=int)\n",
    "\n",
    "    return raw_audio, cvs, labels\n",
    "\n",
    "\n",
    "def label_map(label):\n",
    "    \n",
    "    unique = np.unique(label)\n",
    "    new_labels = np.arange(0, len(unique))\n",
    "    \n",
    "    for i in range(len(unique)):\n",
    "        \n",
    "        comp = unique[i]\n",
    "        \n",
    "        for k in range(len(label)):\n",
    "            if label[k] == comp:\n",
    "                label[k] = new_labels[i]\n",
    "    \n",
    "    return label\n",
    "\n",
    "# Split dataset into data and labels\n",
    "def Split_Data_Label(dataset):\n",
    "    \n",
    "    \n",
    "    data = []\n",
    "    label = []\n",
    "    \n",
    "    for i in range (len(dataset)):\n",
    "        data.append(dataset[i][0])\n",
    "        label.append(dataset[i][1])\n",
    "\n",
    "    \n",
    "    data = np.asarray(data)\n",
    "    label = np.asarray(label)\n",
    "    \n",
    "    return data, label\n",
    "\n",
    "# Split loaded raw_data into folds\n",
    "def Split_Folds(raw_audio, cvs, labels, verbose=False):\n",
    "    '''\n",
    "        Input:\n",
    "            raw_audio: list that contains the raw data\n",
    "            cvs:       list that contains the cross-fold number\n",
    "            labels:    list that contains the category information\n",
    "            verbose:   flag used to print produced folds information\n",
    "        \n",
    "        Output:\n",
    "            f{1,2,3,4,5}:      folds that contains the raw data and labels\n",
    "    '''\n",
    "    \n",
    "    f1 = []\n",
    "    f2 = []\n",
    "    f3 = []\n",
    "    f4 = []\n",
    "    f5 = []\n",
    "    \n",
    "    # Loop over each file audio\n",
    "    for num, audio in enumerate(tqdm(raw_audio)):\n",
    "        \n",
    "        if cvs[num] == 1:\n",
    "            f1.append((audio, labels[num]))\n",
    "        elif cvs[num] == 2:\n",
    "            f2.append([audio, labels[num]])\n",
    "        elif cvs[num] == 3:\n",
    "            f3.append([audio, labels[num]])\n",
    "        elif cvs[num] == 4:\n",
    "            f4.append([audio, labels[num]])\n",
    "        elif cvs[num] == 5:\n",
    "            f5.append([audio, labels[num]])\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    f1 = np.asarray(f1, dtype=object)\n",
    "    f2 = np.asarray(f2, dtype=object)\n",
    "    f3 = np.asarray(f3, dtype=object)\n",
    "    f4 = np.asarray(f4, dtype=object)\n",
    "    f5 = np.asarray(f5, dtype=object)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Folds size: %2d - %2d - %2d - %2d - %2d\" % (len(f1), len(f2), len(f3), len(f4), len(f5)))\n",
    "\n",
    "        print(\"Folds sample shape: \", len(f1[0]))\n",
    "\n",
    "        print(\"Folds sample data shape: \", f1[0][0].shape)\n",
    "        \n",
    "        print(\"Folds sample label type: \", f1[0][1].shape)\n",
    "    \n",
    "    return f1, f2, f3, f4, f5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "settled-sponsorship",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:18<00:00, 107.71it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load raw data\n",
    "PATH = 'audio'\n",
    "raw_files, cvs, labels = Load_ESC10(PATH)\n",
    "labels = label_map(labels)\n",
    "\n",
    "\n",
    "labels = to_categorical(labels, num_classes=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "auburn-termination",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:00<00:00, 399838.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folds size: 80 - 80 - 80 - 80 - 80\n",
      "Folds sample shape:  2\n",
      "Folds sample data shape:  (110250,)\n",
      "Folds sample label type:  (10,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Split the different folds\n",
    "f1, f2, f3, f4, f5 = Split_Folds(raw_files, cvs, labels, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "injured-klein",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Split_Segments(dataset, overlap=0.75, wnd=20480, threshold=10**-6):\n",
    "    \n",
    "    data, label = Split_Data_Label(dataset)\n",
    "\n",
    "    segment_list = []\n",
    "    label_list = []\n",
    "    \n",
    "    # Loop over audio sample\n",
    "    for num, audio in enumerate(tqdm(data)):\n",
    "        for idx in range(0, len(audio) - int(wnd * overlap), int(wnd*(1 - overlap))):\n",
    "\n",
    "            segment = audio[idx:idx+wnd]\n",
    "            \n",
    "            check = np.mean(segment**2)\n",
    "            \n",
    "            if((check>threshold) and (len(segment)==wnd)):\n",
    "                segment_list.append(segment)\n",
    "                label_list.append(label[num])\n",
    "    \n",
    "    #print(len(segment_list))\n",
    "    segment_list = np.asarray(segment_list, dtype=np.float32)\n",
    "    label_list = np.asarray(label_list, dtype=np.float32)\n",
    "    \n",
    "    return segment_list, label_list\n",
    "\n",
    "def Compute_MelSpec3(dataset, bands=60):\n",
    "\n",
    "    features = []\n",
    "    for segment in dataset:\n",
    "        features.append(librosa.core.amplitude_to_db(librosa.feature.melspectrogram(segment, n_mels=bands)))\n",
    "    \n",
    "    log_specgrams = np.asarray(features).reshape(len(features), bands, 41, 1)\n",
    "    features = np.concatenate((log_specgrams, np.zeros(np.shape(log_specgrams)), np.zeros(np.shape(log_specgrams))), axis=3)\n",
    "    \n",
    "    # compute delta_1\n",
    "    for i in range(len(log_specgrams)):\n",
    "        features[i, :, :, 1] = librosa.feature.delta(features[i, :, :, 0])\n",
    "    \n",
    "                              #compute delta_2\n",
    "    for i in range(len(log_specgrams)):\n",
    "        features[i, :, :, 2] = librosa.feature.delta(features[i, :, :, 1])\n",
    "                              \n",
    "    features = features.astype(np.float32)    \n",
    "    return features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cubic-compound",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [00:00<00:00, 2051.21it/s]\n",
      "100%|██████████| 80/80 [00:00<00:00, 2051.38it/s]\n",
      "100%|██████████| 80/80 [00:00<00:00, 2051.38it/s]\n",
      "100%|██████████| 80/80 [00:00<00:00, 2052.70it/s]\n",
      "100%|██████████| 80/80 [00:00<00:00, 2051.32it/s]\n"
     ]
    }
   ],
   "source": [
    "s1, l1 = Split_Segments(f1)\n",
    "s2, l2 = Split_Segments(f2)\n",
    "s3, l3 = Split_Segments(f3)\n",
    "s4, l4 = Split_Segments(f4)\n",
    "s5, l5 = Split_Segments(f5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "brown-stephen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1255, 20480)\n",
      "(1279, 20480)\n",
      "(1283, 20480)\n",
      "(1274, 20480)\n",
      "(1312, 20480)\n"
     ]
    }
   ],
   "source": [
    "print(s1.shape)\n",
    "print(s2.shape)\n",
    "print(s3.shape)\n",
    "print(s4.shape)\n",
    "print(s5.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funky-fight",
   "metadata": {},
   "source": [
    "## Save Different Test-Validation-Test folds file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "legislative-saver",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create set 1\n",
    "train_d = np.concatenate((s1, s2, s3))\n",
    "train_l = np.concatenate((l1, l2, l3))\n",
    "\n",
    "# Create random index for shuffling\n",
    "rnd_indices = np.arange(0, len(train_d))\n",
    "rnd_indices = np.random.shuffle(rnd_indices)\n",
    "\n",
    "# shuffle the sets to decorrelate the segments\n",
    "train_d = train_d[rnd_indices].reshape((len(s1) + len(s2) + len(s3), 20480))\n",
    "train_l = train_l[rnd_indices].reshape((len(s1) + len(s2) + len(s3), 10))\n",
    "\n",
    "\n",
    "# Create random index for shuffling\n",
    "rnd_indices = np.arange(0, len(s4))\n",
    "rnd_indices = np.random.shuffle(rnd_indices)\n",
    "val_d = s4[rnd_indices].reshape(len(s4), 20480)\n",
    "val_l = l4[rnd_indices].reshape(len(s4), 10)\n",
    "\n",
    "# Create random index for shuffling\n",
    "rnd_indices = np.arange(0, len(s5))\n",
    "rnd_indices = np.random.shuffle(rnd_indices)\n",
    "test_d = s5[rnd_indices].reshape(len(s5), 20480)\n",
    "test_l = l5[rnd_indices].reshape(len(s5), 10)\n",
    "\n",
    "# Compute mel specs\n",
    "train_d = Compute_MelSpec3(train_d)\n",
    "val_d = Compute_MelSpec3(val_d)\n",
    "test_d = Compute_MelSpec3(test_d)\n",
    "\n",
    "# Rescale to 0-1\n",
    "train_d = np.interp(train_d, (-100., 150.), (0, 1)).astype(np.float32)\n",
    "val_d = np.interp(val_d, (-100., 150.), (0, 1)).astype(np.float32)\n",
    "test_d = np.interp(test_d, (-100., 150.), (0, 1)).astype(np.float32)\n",
    "\n",
    "# Save data\n",
    "hf = h5py.File('ESC10/MF1.h5', 'w')\n",
    "hf.create_dataset('train_data', data=train_d)\n",
    "hf.create_dataset('train_label', data=train_l)\n",
    "hf.create_dataset('validation_data', data=val_d)\n",
    "hf.create_dataset('validation_label', data=val_l)\n",
    "hf.create_dataset('test_data', data=test_d)\n",
    "hf.create_dataset('test_label', data=test_l)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "blank-serum",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create set 2\n",
    "train_d = np.concatenate((s1, s2, s5))\n",
    "train_l = np.concatenate((l1, l2, l5))\n",
    "\n",
    "# Create random index for shuffling\n",
    "rnd_indices = np.arange(0, len(train_d))\n",
    "rnd_indices = np.random.shuffle(rnd_indices)\n",
    "\n",
    "# shuffle the sets to decorrelate the segments\n",
    "train_d = train_d[rnd_indices].reshape((len(s1) + len(s2) + len(s5), 20480))\n",
    "train_l = train_l[rnd_indices].reshape((len(s1) + len(s2) + len(s5), 10))\n",
    "\n",
    "\n",
    "# Create random index for shuffling\n",
    "rnd_indices = np.arange(0, len(s3))\n",
    "rnd_indices = np.random.shuffle(rnd_indices)\n",
    "val_d = s3[rnd_indices].reshape(len(s3), 20480)\n",
    "val_l = l3[rnd_indices].reshape(len(s3), 10)\n",
    "\n",
    "# Create random index for shuffling\n",
    "rnd_indices = np.arange(0, len(s4))\n",
    "rnd_indices = np.random.shuffle(rnd_indices)\n",
    "test_d = s4[rnd_indices].reshape(len(s4), 20480)\n",
    "test_l = l4[rnd_indices].reshape(len(s4), 10)\n",
    "\n",
    "# Compute mel specs\n",
    "train_d = Compute_MelSpec3(train_d)\n",
    "val_d = Compute_MelSpec3(val_d)\n",
    "test_d = Compute_MelSpec3(test_d)\n",
    "\n",
    "# Rescale to 0-1\n",
    "train_d = np.interp(train_d, (-100., 150.), (0, 1)).astype(np.float32)\n",
    "val_d = np.interp(val_d, (-100., 150.), (0, 1)).astype(np.float32)\n",
    "test_d = np.interp(test_d, (-100., 150.), (0, 1)).astype(np.float32)\n",
    "\n",
    "hf = h5py.File('ESC10/MF2.h5', 'w')\n",
    "hf.create_dataset('train_data', data=train_d)\n",
    "hf.create_dataset('train_label', data=train_l)\n",
    "hf.create_dataset('validation_data', data=val_d)\n",
    "hf.create_dataset('validation_label', data=val_l)\n",
    "hf.create_dataset('test_data', data=test_d)\n",
    "hf.create_dataset('test_label', data=test_l)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "economic-diana",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create set 3\n",
    "train_d = np.concatenate((s1, s4, s5))\n",
    "train_l = np.concatenate((l1, l4, l5))\n",
    "\n",
    "# Create random index for shuffling\n",
    "rnd_indices = np.arange(0, len(train_d))\n",
    "rnd_indices = np.random.shuffle(rnd_indices)\n",
    "\n",
    "# shuffle the sets to decorrelate the segments\n",
    "train_d = train_d[rnd_indices].reshape((len(s1) + len(s4) + len(s5), 20480))\n",
    "train_l = train_l[rnd_indices].reshape((len(s1) + len(s4) + len(s5), 10))\n",
    "\n",
    "\n",
    "# Create random index for shuffling\n",
    "rnd_indices = np.arange(0, len(s2))\n",
    "rnd_indices = np.random.shuffle(rnd_indices)\n",
    "val_d = s2[rnd_indices].reshape(len(s2), 20480)\n",
    "val_l = l2[rnd_indices].reshape(len(s2), 10)\n",
    "\n",
    "# Create random index for shuffling\n",
    "rnd_indices = np.arange(0, len(s3))\n",
    "rnd_indices = np.random.shuffle(rnd_indices)\n",
    "test_d = s3[rnd_indices].reshape(len(s3), 20480)\n",
    "test_l = l3[rnd_indices].reshape(len(s3), 10)\n",
    "\n",
    "# Compute mel specs\n",
    "train_d = Compute_MelSpec3(train_d)\n",
    "val_d = Compute_MelSpec3(val_d)\n",
    "test_d = Compute_MelSpec3(test_d)\n",
    "\n",
    "# Rescale to 0-1\n",
    "train_d = np.interp(train_d, (-100., 150.), (0, 1)).astype(np.float32)\n",
    "val_d = np.interp(val_d, (-100., 150.), (0, 1)).astype(np.float32)\n",
    "test_d = np.interp(test_d, (-100., 150.), (0, 1)).astype(np.float32)\n",
    "\n",
    "hf = h5py.File('ESC10/MF3.h5', 'w')\n",
    "hf.create_dataset('train_data', data=train_d)\n",
    "hf.create_dataset('train_label', data=train_l)\n",
    "hf.create_dataset('validation_data', data=val_d)\n",
    "hf.create_dataset('validation_label', data=val_l)\n",
    "hf.create_dataset('test_data', data=test_d)\n",
    "hf.create_dataset('test_label', data=test_l)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "governing-control",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create set 4\n",
    "train_d = np.concatenate((s3, s4, s5))\n",
    "train_l = np.concatenate((l3, l4, l5))\n",
    "\n",
    "# Create random index for shuffling\n",
    "rnd_indices = np.arange(0, len(train_d))\n",
    "rnd_indices = np.random.shuffle(rnd_indices)\n",
    "\n",
    "# shuffle the sets to decorrelate the segments\n",
    "train_d = train_d[rnd_indices].reshape((len(s3) + len(s4) + len(s5), 20480))\n",
    "train_l = train_l[rnd_indices].reshape((len(s3) + len(s4) + len(s5), 10))\n",
    "\n",
    "\n",
    "# Create random index for shuffling\n",
    "rnd_indices = np.arange(0, len(s1))\n",
    "rnd_indices = np.random.shuffle(rnd_indices)\n",
    "val_d = s1[rnd_indices].reshape(len(s1), 20480)\n",
    "val_l = l1[rnd_indices].reshape(len(s1), 10)\n",
    "\n",
    "# Create random index for shuffling\n",
    "rnd_indices = np.arange(0, len(s5))\n",
    "rnd_indices = np.random.shuffle(rnd_indices)\n",
    "test_d = s2[rnd_indices].reshape(len(s2), 20480)\n",
    "test_l = l2[rnd_indices].reshape(len(s2), 10)\n",
    "\n",
    "# Compute mel specs\n",
    "train_d = Compute_MelSpec3(train_d)\n",
    "val_d = Compute_MelSpec3(val_d)\n",
    "test_d = Compute_MelSpec3(test_d)\n",
    "\n",
    "# Rescale to 0-1\n",
    "train_d = np.interp(train_d, (-100., 150.), (0, 1)).astype(np.float32)\n",
    "val_d = np.interp(val_d, (-100., 150.), (0, 1)).astype(np.float32)\n",
    "test_d = np.interp(test_d, (-100., 150.), (0, 1)).astype(np.float32)\n",
    "\n",
    "hf = h5py.File('ESC10/MF4.h5', 'w')\n",
    "hf.create_dataset('train_data', data=train_d)\n",
    "hf.create_dataset('train_label', data=train_l)\n",
    "hf.create_dataset('validation_data', data=val_d)\n",
    "hf.create_dataset('validation_label', data=val_l)\n",
    "hf.create_dataset('test_data', data=test_d)\n",
    "hf.create_dataset('test_label', data=test_l)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "invisible-colleague",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create set 5\n",
    "train_d = np.concatenate((s4, s2, s3))\n",
    "train_l = np.concatenate((l4, l2, l3))\n",
    "\n",
    "# Create random index for shuffling\n",
    "rnd_indices = np.arange(0, len(train_d))\n",
    "rnd_indices = np.random.shuffle(rnd_indices)\n",
    "\n",
    "# shuffle the sets to decorrelate the segments\n",
    "train_d = train_d[rnd_indices].reshape((len(s4) + len(s2) + len(s3), 20480))\n",
    "train_l = train_l[rnd_indices].reshape((len(s4) + len(s2) + len(s3), 10))\n",
    "\n",
    "\n",
    "# Create random index for shuffling\n",
    "rnd_indices = np.arange(0, len(s5))\n",
    "rnd_indices = np.random.shuffle(rnd_indices)\n",
    "val_d = s5[rnd_indices].reshape(len(s5), 20480)\n",
    "val_l = l5[rnd_indices].reshape(len(s5), 10)\n",
    "\n",
    "# Create random index for shuffling\n",
    "rnd_indices = np.arange(0, len(s1))\n",
    "rnd_indices = np.random.shuffle(rnd_indices)\n",
    "test_d = s1[rnd_indices].reshape(len(s1), 20480)\n",
    "test_l = l1[rnd_indices].reshape(len(s1), 10)\n",
    "\n",
    "# Compute mel specs\n",
    "train_d = Compute_MelSpec3(train_d)\n",
    "val_d = Compute_MelSpec3(val_d)\n",
    "test_d = Compute_MelSpec3(test_d)\n",
    "\n",
    "# Rescale to 0-1\n",
    "train_d = np.interp(train_d, (-100., 150.), (0, 1)).astype(np.float32)\n",
    "val_d = np.interp(val_d, (-100., 150.), (0, 1)).astype(np.float32)\n",
    "test_d = np.interp(test_d, (-100., 150.), (0, 1)).astype(np.float32)\n",
    "\n",
    "hf = h5py.File('ESC10/MF5.h5', 'w')\n",
    "hf.create_dataset('train_data', data=train_d)\n",
    "hf.create_dataset('train_label', data=train_l)\n",
    "hf.create_dataset('validation_data', data=val_d)\n",
    "hf.create_dataset('validation_label', data=val_l)\n",
    "hf.create_dataset('test_data', data=test_d)\n",
    "hf.create_dataset('test_label', data=test_l)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "yellow-specialist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n"
     ]
    }
   ],
   "source": [
    "print(train_d[0][0][0].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "checked-commerce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
