{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "vanilla-vulnerability",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import h5py\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display\n",
    "import librosa.display\n",
    "\n",
    "# Use GPU\n",
    "from tensorflow.python.client import device_lib \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "indian-webmaster",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw data\n",
    "def Load_RAW(path):\n",
    "    '''\n",
    "        Input:\n",
    "            path: folder of the dataset\n",
    "        \n",
    "        Output:\n",
    "            raw_data:  list that contains the raw data\n",
    "            cvs:       list that contains the cross-fold number\n",
    "            labels:    list that contains the category information\n",
    "    '''\n",
    "    \n",
    "    # Container for the dataset\n",
    "    raw_data = []\n",
    "    cvs = []\n",
    "    labels = []\n",
    "    # Load every file inside the folder\n",
    "    for file_name in tqdm(os.listdir(path)):\n",
    "\n",
    "        try:\n",
    "            # Get audio data and sampling rate\n",
    "            audio, sampling_rate = librosa.load(os.path.join(path, file_name), res_type='kaiser_fast')\n",
    "            # Split the file name\n",
    "            name_splitted = re.split('[-.]', file_name)\n",
    "            \n",
    "            # Append a row of 3 elements\n",
    "            raw_data.append(audio)\n",
    "            cvs.append(name_splitted[0])\n",
    "            labels.append(name_splitted[3])\n",
    "        except Exception as e:\n",
    "            pass\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    raw_audio = np.asarray(raw_data)\n",
    "    cvs = np.asarray(cvs, dtype=int)\n",
    "    labels = np.asarray(labels, dtype=int)\n",
    "    \n",
    "    # onehot encode the labels in 50 classes\n",
    "    onehot_labels = to_categorical(labels, num_classes=50)\n",
    "    \n",
    "    return raw_audio, cvs, onehot_labels\n",
    "\n",
    "\n",
    "# Split dataset into data and labels\n",
    "def Split_Data_Label(dataset):\n",
    "    \n",
    "    \n",
    "    data = []\n",
    "    label = []\n",
    "    \n",
    "    for i in range (len(dataset)):\n",
    "        data.append(dataset[i][0])\n",
    "        label.append(dataset[i][1])\n",
    "\n",
    "    \n",
    "    data = np.asarray(data)\n",
    "    label = np.asarray(label)\n",
    "    \n",
    "    return data, label\n",
    "\n",
    "# Split loaded raw_data into folds\n",
    "def Split_Folds(raw_audio, cvs, labels, verbose=False):\n",
    "    '''\n",
    "        Input:\n",
    "            raw_audio: list that contains the raw data\n",
    "            cvs:       list that contains the cross-fold number\n",
    "            labels:    list that contains the category information\n",
    "            verbose:   flag used to print produced folds information\n",
    "        \n",
    "        Output:\n",
    "            f{1,2,3,4,5}:      folds that contains the raw data and labels\n",
    "    '''\n",
    "    \n",
    "    f1 = []\n",
    "    f2 = []\n",
    "    f3 = []\n",
    "    f4 = []\n",
    "    f5 = []\n",
    "    \n",
    "    # Loop over each file audio\n",
    "    for num, audio in enumerate(tqdm(raw_audio)):\n",
    "        \n",
    "        if cvs[num] == 1:\n",
    "            f1.append((audio, labels[num]))\n",
    "        elif cvs[num] == 2:\n",
    "            f2.append([audio, labels[num]])\n",
    "        elif cvs[num] == 3:\n",
    "            f3.append([audio, labels[num]])\n",
    "        elif cvs[num] == 4:\n",
    "            f4.append([audio, labels[num]])\n",
    "        elif cvs[num] == 5:\n",
    "            f5.append([audio, labels[num]])\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    f1 = np.asarray(f1, dtype=object)\n",
    "    f2 = np.asarray(f2, dtype=object)\n",
    "    f3 = np.asarray(f3, dtype=object)\n",
    "    f4 = np.asarray(f4, dtype=object)\n",
    "    f5 = np.asarray(f5, dtype=object)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Folds size: %2d - %2d - %2d - %2d - %2d\" % (len(f1), len(f2), len(f3), len(f4), len(f5)))\n",
    "\n",
    "        print(\"Folds sample shape: \", len(f1[0]))\n",
    "\n",
    "        print(\"Folds sample data shape: \", f1[0][0].shape)\n",
    "        \n",
    "        print(\"Folds sample label type: \", f1[0][1].shape)\n",
    "    \n",
    "    return f1, f2, f3, f4, f5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "widespread-behalf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:27<00:00, 22.97it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load raw data\n",
    "PATH = 'audio'\n",
    "raw_files, cvs, labels = Load_RAW(PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dutch-purse",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:00<00:00, 500006.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folds size: 400 - 400 - 400 - 400 - 400\n",
      "Folds sample shape:  2\n",
      "Folds sample data shape:  (110250,)\n",
      "Folds sample label type:  (50,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Split the different folds\n",
    "f1, f2, f3, f4, f5 = Split_Folds(raw_files, cvs, labels, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "interstate-vatican",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Split_Segments(dataset, overlap=0.75, wnd=20480, threshold=10**-6):\n",
    "    \n",
    "    data, label = Split_Data_Label(dataset)\n",
    "\n",
    "    segment_list = []\n",
    "    label_list = []\n",
    "    \n",
    "    # Loop over audio sample\n",
    "    for num, audio in enumerate(tqdm(data)):\n",
    "        for idx in range(0, len(audio) - int(wnd * overlap), int(wnd*(1 - overlap))):\n",
    "\n",
    "            segment = audio[idx:idx+wnd]\n",
    "            \n",
    "            check = np.mean(segment**2)\n",
    "            \n",
    "            if((check>threshold) and (len(segment)==wnd)):\n",
    "                segment_list.append(segment)\n",
    "                label_list.append(label[num])\n",
    "    \n",
    "    #print(len(segment_list))\n",
    "    segment_list = np.asarray(segment_list, dtype=np.float32)\n",
    "    label_list = np.asarray(label_list, dtype=np.float32)\n",
    "    \n",
    "    return segment_list, label_list\n",
    "\n",
    "def Compute_MelSpec3(dataset, bands=60):\n",
    "\n",
    "    features = []\n",
    "    for segment in dataset:\n",
    "        features.append(librosa.core.amplitude_to_db(librosa.feature.melspectrogram(segment, n_mels=bands)))\n",
    "    \n",
    "    log_specgrams = np.asarray(features).reshape(len(features), bands, 41, 1)\n",
    "    features = np.concatenate((log_specgrams, np.zeros(np.shape(log_specgrams)), np.zeros(np.shape(log_specgrams))), axis=3)\n",
    "    \n",
    "    # compute delta_1\n",
    "    for i in range(len(log_specgrams)):\n",
    "        features[i, :, :, 1] = librosa.feature.delta(features[i, :, :, 0])\n",
    "    \n",
    "                              #compute delta_2\n",
    "    for i in range(len(log_specgrams)):\n",
    "        features[i, :, :, 2] = librosa.feature.delta(features[i, :, :, 1])\n",
    "                              \n",
    "    features = features.astype(np.float32)    \n",
    "    return features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dried-terrorism",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:00<00:00, 2062.15it/s]\n",
      "100%|██████████| 400/400 [00:00<00:00, 2139.06it/s]\n",
      "100%|██████████| 400/400 [00:00<00:00, 2138.76it/s]\n",
      "100%|██████████| 400/400 [00:00<00:00, 2093.98it/s]\n",
      "100%|██████████| 400/400 [00:00<00:00, 2072.82it/s]\n"
     ]
    }
   ],
   "source": [
    "s1, l1 = Split_Segments(f1)\n",
    "s2, l2 = Split_Segments(f2)\n",
    "s3, l3 = Split_Segments(f3)\n",
    "s4, l4 = Split_Segments(f4)\n",
    "s5, l5 = Split_Segments(f5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "established-protein",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6438, 20480)\n",
      "(6467, 20480)\n",
      "(6387, 20480)\n",
      "(6571, 20480)\n",
      "(6521, 20480)\n"
     ]
    }
   ],
   "source": [
    "print(s1.shape)\n",
    "print(s2.shape)\n",
    "print(s3.shape)\n",
    "print(s4.shape)\n",
    "print(s5.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driving-region",
   "metadata": {},
   "source": [
    "## Save Different Test-Validation-Test folds file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "greek-flight",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create set 1\n",
    "train_d = np.concatenate((s1, s2, s3))\n",
    "train_l = np.concatenate((l1, l2, l3))\n",
    "\n",
    "# Create random index for shuffling\n",
    "rnd_indices = np.arange(0, len(train_d))\n",
    "rnd_indices = np.random.shuffle(rnd_indices)\n",
    "\n",
    "# shuffle the sets to decorrelate the segments\n",
    "train_d = train_d[rnd_indices].reshape((len(s1) + len(s2) + len(s3), 20480))\n",
    "train_l = train_l[rnd_indices].reshape((len(s1) + len(s2) + len(s3), 50))\n",
    "\n",
    "\n",
    "# Create random index for shuffling\n",
    "rnd_indices = np.arange(0, len(s4))\n",
    "rnd_indices = np.random.shuffle(rnd_indices)\n",
    "val_d = s4[rnd_indices].reshape(len(s4), 20480)\n",
    "val_l = l4[rnd_indices].reshape(len(s4), 50)\n",
    "\n",
    "# Create random index for shuffling\n",
    "rnd_indices = np.arange(0, len(s5))\n",
    "rnd_indices = np.random.shuffle(rnd_indices)\n",
    "test_d = s5[rnd_indices].reshape(len(s5), 20480)\n",
    "test_l = l5[rnd_indices].reshape(len(s5), 50)\n",
    "\n",
    "# Compute mel specs\n",
    "train_d = Compute_MelSpec3(train_d)\n",
    "val_d = Compute_MelSpec3(val_d)\n",
    "test_d = Compute_MelSpec3(test_d)\n",
    "\n",
    "# Rescale to 0-1\n",
    "train_d = np.interp(train_d, (-100., 150.), (0, 1)).astype(np.float32)\n",
    "val_d = np.interp(val_d, (-100., 150.), (0, 1)).astype(np.float32)\n",
    "test_d = np.interp(test_d, (-100., 150.), (0, 1)).astype(np.float32)\n",
    "\n",
    "hf = h5py.File('ESC50/MF1.h5', 'w')\n",
    "hf.create_dataset('train_data', data=train_d)\n",
    "hf.create_dataset('train_label', data=train_l)\n",
    "hf.create_dataset('validation_data', data=val_d)\n",
    "hf.create_dataset('validation_label', data=val_l)\n",
    "hf.create_dataset('test_data', data=test_d)\n",
    "hf.create_dataset('test_label', data=test_l)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "spoken-camping",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create set 2\n",
    "train_d = np.concatenate((s1, s2, s5))\n",
    "train_l = np.concatenate((l1, l2, l5))\n",
    "\n",
    "# Create random index for shuffling\n",
    "rnd_indices = np.arange(0, len(train_d))\n",
    "rnd_indices = np.random.shuffle(rnd_indices)\n",
    "\n",
    "# shuffle the sets to decorrelate the segments\n",
    "train_d = train_d[rnd_indices].reshape((len(s1) + len(s2) + len(s5), 20480))\n",
    "train_l = train_l[rnd_indices].reshape((len(s1) + len(s2) + len(s5), 50))\n",
    "\n",
    "\n",
    "# Create random index for shuffling\n",
    "rnd_indices = np.arange(0, len(s3))\n",
    "rnd_indices = np.random.shuffle(rnd_indices)\n",
    "val_d = s3[rnd_indices].reshape(len(s3), 20480)\n",
    "val_l = l3[rnd_indices].reshape(len(s3), 50)\n",
    "\n",
    "# Create random index for shuffling\n",
    "rnd_indices = np.arange(0, len(s4))\n",
    "rnd_indices = np.random.shuffle(rnd_indices)\n",
    "test_d = s4[rnd_indices].reshape(len(s4), 20480)\n",
    "test_l = l4[rnd_indices].reshape(len(s4), 50)\n",
    "\n",
    "# Compute mel specs\n",
    "train_d = Compute_MelSpec3(train_d)\n",
    "val_d = Compute_MelSpec3(val_d)\n",
    "test_d = Compute_MelSpec3(test_d)\n",
    "\n",
    "# Rescale to 0-1\n",
    "train_d = np.interp(train_d, (-100., 150.), (0, 1)).astype(np.float32)\n",
    "val_d = np.interp(val_d, (-100., 150.), (0, 1)).astype(np.float32)\n",
    "test_d = np.interp(test_d, (-100., 150.), (0, 1)).astype(np.float32)\n",
    "\n",
    "hf = h5py.File('ESC50/MF2.h5', 'w')\n",
    "hf.create_dataset('train_data', data=train_d)\n",
    "hf.create_dataset('train_label', data=train_l)\n",
    "hf.create_dataset('validation_data', data=val_d)\n",
    "hf.create_dataset('validation_label', data=val_l)\n",
    "hf.create_dataset('test_data', data=test_d)\n",
    "hf.create_dataset('test_label', data=test_l)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "earned-individual",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create set 3\n",
    "train_d = np.concatenate((s1, s4, s5))\n",
    "train_l = np.concatenate((l1, l4, l5))\n",
    "\n",
    "# Create random index for shuffling\n",
    "rnd_indices = np.arange(0, len(train_d))\n",
    "rnd_indices = np.random.shuffle(rnd_indices)\n",
    "\n",
    "# shuffle the sets to decorrelate the segments\n",
    "train_d = train_d[rnd_indices].reshape((len(s1) + len(s4) + len(s5), 20480))\n",
    "train_l = train_l[rnd_indices].reshape((len(s1) + len(s4) + len(s5), 50))\n",
    "\n",
    "\n",
    "# Create random index for shuffling\n",
    "rnd_indices = np.arange(0, len(s2))\n",
    "rnd_indices = np.random.shuffle(rnd_indices)\n",
    "val_d = s2[rnd_indices].reshape(len(s2), 20480)\n",
    "val_l = l2[rnd_indices].reshape(len(s2), 50)\n",
    "\n",
    "# Create random index for shuffling\n",
    "rnd_indices = np.arange(0, len(s3))\n",
    "rnd_indices = np.random.shuffle(rnd_indices)\n",
    "test_d = s3[rnd_indices].reshape(len(s3), 20480)\n",
    "test_l = l3[rnd_indices].reshape(len(s3), 50)\n",
    "\n",
    "# Compute mel specs\n",
    "train_d = Compute_MelSpec3(train_d)\n",
    "val_d = Compute_MelSpec3(val_d)\n",
    "test_d = Compute_MelSpec3(test_d)\n",
    "\n",
    "# Rescale to 0-1\n",
    "train_d = np.interp(train_d, (-100., 150.), (0, 1)).astype(np.float32)\n",
    "val_d = np.interp(val_d, (-100., 150.), (0, 1)).astype(np.float32)\n",
    "test_d = np.interp(test_d, (-100., 150.), (0, 1)).astype(np.float32)\n",
    "\n",
    "hf = h5py.File('ESC50/MF3.h5', 'w')\n",
    "hf.create_dataset('train_data', data=train_d)\n",
    "hf.create_dataset('train_label', data=train_l)\n",
    "hf.create_dataset('validation_data', data=val_d)\n",
    "hf.create_dataset('validation_label', data=val_l)\n",
    "hf.create_dataset('test_data', data=test_d)\n",
    "hf.create_dataset('test_label', data=test_l)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "furnished-phone",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create set 4\n",
    "train_d = np.concatenate((s3, s4, s5))\n",
    "train_l = np.concatenate((l3, l4, l5))\n",
    "\n",
    "# Create random index for shuffling\n",
    "rnd_indices = np.arange(0, len(train_d))\n",
    "rnd_indices = np.random.shuffle(rnd_indices)\n",
    "\n",
    "# shuffle the sets to decorrelate the segments\n",
    "train_d = train_d[rnd_indices].reshape((len(s3) + len(s4) + len(s5), 20480))\n",
    "train_l = train_l[rnd_indices].reshape((len(s3) + len(s4) + len(s5), 50))\n",
    "\n",
    "\n",
    "# Create random index for shuffling\n",
    "rnd_indices = np.arange(0, len(s1))\n",
    "rnd_indices = np.random.shuffle(rnd_indices)\n",
    "val_d = s1[rnd_indices].reshape(len(s1), 20480)\n",
    "val_l = l1[rnd_indices].reshape(len(s1), 50)\n",
    "\n",
    "# Create random index for shuffling\n",
    "rnd_indices = np.arange(0, len(s5))\n",
    "rnd_indices = np.random.shuffle(rnd_indices)\n",
    "test_d = s2[rnd_indices].reshape(len(s2), 20480)\n",
    "test_l = l2[rnd_indices].reshape(len(s2), 50)\n",
    "\n",
    "# Compute mel specs\n",
    "train_d = Compute_MelSpec3(train_d)\n",
    "val_d = Compute_MelSpec3(val_d)\n",
    "test_d = Compute_MelSpec3(test_d)\n",
    "\n",
    "# Rescale to 0-1\n",
    "train_d = np.interp(train_d, (-100., 150.), (0, 1)).astype(np.float32)\n",
    "val_d = np.interp(val_d, (-100., 150.), (0, 1)).astype(np.float32)\n",
    "test_d = np.interp(test_d, (-100., 150.), (0, 1)).astype(np.float32)\n",
    "\n",
    "hf = h5py.File('ESC50/MF4.h5', 'w')\n",
    "hf.create_dataset('train_data', data=train_d)\n",
    "hf.create_dataset('train_label', data=train_l)\n",
    "hf.create_dataset('validation_data', data=val_d)\n",
    "hf.create_dataset('validation_label', data=val_l)\n",
    "hf.create_dataset('test_data', data=test_d)\n",
    "hf.create_dataset('test_label', data=test_l)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "straight-bookmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create set 5\n",
    "train_d = np.concatenate((s4, s2, s3))\n",
    "train_l = np.concatenate((l4, l2, l3))\n",
    "\n",
    "# Create random index for shuffling\n",
    "rnd_indices = np.arange(0, len(train_d))\n",
    "rnd_indices = np.random.shuffle(rnd_indices)\n",
    "\n",
    "# shuffle the sets to decorrelate the segments\n",
    "train_d = train_d[rnd_indices].reshape((len(s4) + len(s2) + len(s3), 20480))\n",
    "train_l = train_l[rnd_indices].reshape((len(s4) + len(s2) + len(s3), 50))\n",
    "\n",
    "\n",
    "# Create random index for shuffling\n",
    "rnd_indices = np.arange(0, len(s5))\n",
    "rnd_indices = np.random.shuffle(rnd_indices)\n",
    "val_d = s5[rnd_indices].reshape(len(s5), 20480)\n",
    "val_l = l5[rnd_indices].reshape(len(s5), 50)\n",
    "\n",
    "# Create random index for shuffling\n",
    "rnd_indices = np.arange(0, len(s1))\n",
    "rnd_indices = np.random.shuffle(rnd_indices)\n",
    "test_d = s1[rnd_indices].reshape(len(s1), 20480)\n",
    "test_l = l1[rnd_indices].reshape(len(s1), 50)\n",
    "\n",
    "# Compute mel specs\n",
    "train_d = Compute_MelSpec3(train_d)\n",
    "val_d = Compute_MelSpec3(val_d)\n",
    "test_d = Compute_MelSpec3(test_d)\n",
    "\n",
    "# Rescale to 0-1\n",
    "train_d = np.interp(train_d, (-100., 150.), (0, 1)).astype(np.float32)\n",
    "val_d = np.interp(val_d, (-100., 150.), (0, 1)).astype(np.float32)\n",
    "test_d = np.interp(test_d, (-100., 150.), (0, 1)).astype(np.float32)\n",
    "\n",
    "hf = h5py.File('ESC50/MF5.h5', 'w')\n",
    "hf.create_dataset('train_data', data=train_d)\n",
    "hf.create_dataset('train_label', data=train_l)\n",
    "hf.create_dataset('validation_data', data=val_d)\n",
    "hf.create_dataset('validation_label', data=val_l)\n",
    "hf.create_dataset('test_data', data=test_d)\n",
    "hf.create_dataset('test_label', data=test_l)\n",
    "hf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "characteristic-shark",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n"
     ]
    }
   ],
   "source": [
    "print(train_d[0, 0, 0, 0].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automatic-tuning",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
