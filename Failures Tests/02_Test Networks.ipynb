{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "signal-nightmare",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 15190738222272437095\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 3200397649557572826\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 7046801664\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 16373545511013765600\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 9999579160609133284\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import h5py\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display\n",
    "import librosa.display\n",
    "\n",
    "# Use GPU\n",
    "from tensorflow.python.client import device_lib \n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "circular-registrar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw data\n",
    "def Load_RAW(path):\n",
    "    '''\n",
    "        Input:\n",
    "            path: folder of the dataset\n",
    "        \n",
    "        Output:\n",
    "            raw_data:  list that contains the raw data\n",
    "            cvs:       list that contains the cross-fold number\n",
    "            labels:    list that contains the category information\n",
    "    '''\n",
    "    \n",
    "    # Container for the dataset\n",
    "    raw_data = []\n",
    "    cvs = []\n",
    "    labels = []\n",
    "    # Load every file inside the folder\n",
    "    for file_name in tqdm(os.listdir(path)):\n",
    "\n",
    "        try:\n",
    "            # Get audio data and sampling rate\n",
    "            audio, sampling_rate = librosa.load(os.path.join(path, file_name), res_type='kaiser_fast')\n",
    "            # Split the file name\n",
    "            name_splitted = re.split('[-.]', file_name)\n",
    "            \n",
    "            # Append a row of 3 elements\n",
    "            raw_data.append(audio)\n",
    "            cvs.append(name_splitted[0])\n",
    "            labels.append(name_splitted[3])\n",
    "        except Exception as e:\n",
    "            pass\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    raw_audio = np.asarray(raw_data)\n",
    "    cvs = np.asarray(cvs, dtype=int)\n",
    "    labels = np.asarray(labels, dtype=int)\n",
    "    \n",
    "    # onehot encode the labels in 50 classes\n",
    "    onehot_labels = to_categorical(labels, num_classes=50)\n",
    "    \n",
    "    return raw_audio, cvs, onehot_labels\n",
    "\n",
    "# Load saved data\n",
    "def Load_Augmented(name='', path='Augmented_Data/'):\n",
    "    '''\n",
    "        Input:\n",
    "            name:      name of the file\n",
    "            path:      path of the file\n",
    "        \n",
    "        Output:\n",
    "            dataset:   loaded dataset with data and labels\n",
    "    '''\n",
    "    hf = h5py.File(path + name + '.h5', 'r')\n",
    "    data =  np.array(hf.get('data'))\n",
    "    labels = np.array(hf.get('label'))\n",
    "    hf.close()\n",
    "    \n",
    "    data = np.asarray(data, dtype=np.float32)\n",
    "    label = np.asarray(labels, dtype=np.float32)\n",
    "    '''\n",
    "    dataset = []\n",
    "    for i in range(len(data)):\n",
    "        dataset.append([data[i], labels[i]])\n",
    "    \n",
    "    dataset = np.asarray(dataset, dtype=object)\n",
    "    '''\n",
    "    return data, labels\n",
    "\n",
    "af1, alf1 = Load_Augmented(name='af1', path='Augmented_Data/')\n",
    "\n",
    "def Split_Folds(raw_audio, cvs, labels, verbose=False):\n",
    "    '''\n",
    "        Input:\n",
    "            raw_audio: list that contains the raw data\n",
    "            cvs:       list that contains the cross-fold number\n",
    "            labels:    list that contains the category information\n",
    "            verbose:   flag used to print produced folds information\n",
    "        \n",
    "        Output:\n",
    "            f{1,2,3,4,5}:      folds that contains the raw data and labels\n",
    "    '''\n",
    "    \n",
    "    f1 = []\n",
    "    f2 = []\n",
    "    f3 = []\n",
    "    f4 = []\n",
    "    f5 = []\n",
    "    \n",
    "    # Loop over each file audio\n",
    "    for num, audio in enumerate(tqdm(raw_audio)):\n",
    "        \n",
    "        if cvs[num] == 1:\n",
    "            f1.append((audio, labels[num]))\n",
    "        elif cvs[num] == 2:\n",
    "            f2.append([audio, labels[num]])\n",
    "        elif cvs[num] == 3:\n",
    "            f3.append([audio, labels[num]])\n",
    "        elif cvs[num] == 4:\n",
    "            f4.append([audio, labels[num]])\n",
    "        elif cvs[num] == 5:\n",
    "            f5.append([audio, labels[num]])\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    f1 = np.asarray(f1, dtype=object)\n",
    "    f2 = np.asarray(f2, dtype=object)\n",
    "    f3 = np.asarray(f3, dtype=object)\n",
    "    f4 = np.asarray(f4, dtype=object)\n",
    "    f5 = np.asarray(f5, dtype=object)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Folds size: %2d - %2d - %2d - %2d - %2d\" % (len(f1), len(f2), len(f3), len(f4), len(f5)))\n",
    "\n",
    "        print(\"Folds sample shape: \", len(f1[0]))\n",
    "\n",
    "        print(\"Folds sample data shape: \", f1[0][0].shape)\n",
    "        \n",
    "        print(\"Folds sample label type: \", f1[0][1].shape)\n",
    "    \n",
    "    return f1, f2, f3, f4, f5\n",
    "\n",
    "\n",
    "def Split_Data_Label(dataset):\n",
    "    \n",
    "    \n",
    "    data = []\n",
    "    label = []\n",
    "    \n",
    "    for i in range (len(dataset)):\n",
    "        data.append(dataset[i][0])\n",
    "        label.append(dataset[i][1])\n",
    "\n",
    "    \n",
    "    data = np.asarray(data)\n",
    "    label = np.asarray(label)\n",
    "    \n",
    "    return data, label\n",
    "\n",
    "\n",
    "# Load saved data\n",
    "def Load_Augmented(name='', path='Augmented_Data/'):\n",
    "    '''\n",
    "        Input:\n",
    "            name:      name of the file\n",
    "            path:      path of the file\n",
    "        \n",
    "        Output:\n",
    "            dataset:   loaded dataset with data and labels\n",
    "    '''\n",
    "    hf = h5py.File(path + name + '.h5', 'r')\n",
    "    data =  np.array(hf.get('data'))\n",
    "    labels = np.array(hf.get('label'))\n",
    "    hf.close()\n",
    "    \n",
    "    data = np.asarray(data, dtype=np.float32)\n",
    "    label = np.asarray(labels, dtype=np.float32)\n",
    "\n",
    "    return data, labels\n",
    "\n",
    "def Preprocessing(raw_audio, labels, bands=60, frames=41):\n",
    "    '''\n",
    "        Input:\n",
    "            raw_audio:     list that contains the raw/augmented data\n",
    "            labels:        list that contains the category information\n",
    "            bands:         number of mel band to use\n",
    "            frames:        number of frames to use\n",
    "        \n",
    "        Output:\n",
    "            features:      numpy array that contains processed audio data with log-melspec and delta\n",
    "            new_labels:    new labels for each augmented segment\n",
    "    '''    \n",
    "    \n",
    "    new_labels = []\n",
    "    augmented_spec = []\n",
    "    \n",
    "    # Normalize the raw data\n",
    "    norm_factor = np.percentile(raw_audio, 99) - np.percentile(raw_audio, 5)\n",
    "    raw_audio = raw_audio / norm_factor\n",
    "    \n",
    "    # Loop over each file audio\n",
    "    for num, audio in enumerate(tqdm(raw_audio)):\n",
    "    \n",
    "        # Convert audio to melspectogram\n",
    "        '''\n",
    "            With default n_fft=2048 we have the filter size of 2048/2+1=1025 [Nyquist Frequency]\n",
    "        '''\n",
    "        melspec = librosa.feature.melspectrogram(audio, n_mels=bands, hop_length=512)\n",
    "        \n",
    "        # Convert melspec to log melspec\n",
    "        logspec = librosa.core.amplitude_to_db(melspec)\n",
    "        \n",
    "        counter = 0\n",
    "        # Spectrogram splitting with 50% overlap and adapt cv-fold and labels info\n",
    "        for idx in range(0, len(logspec[0]) - frames, int(frames/2)):\n",
    "            augmented_spec.append(logspec[:, idx:idx+frames])\n",
    "            new_labels.append(labels[num])\n",
    "            counter = counter +1\n",
    "            \n",
    "    # Reshape the outputs\n",
    "    log_specgrams = np.asarray(augmented_spec).reshape(len(augmented_spec), bands, frames, 1)\n",
    "    features = np.concatenate((log_specgrams, np.zeros(np.shape(log_specgrams))), axis=3)\n",
    "    new_labels = np.asarray(new_labels)\n",
    "    \n",
    "    # Fill the delta features\n",
    "    for i in range(len(log_specgrams)):\n",
    "        features[i, :, :, 1] = librosa.feature.delta(features[i, :, :, 0])\n",
    "    \n",
    "    ######  Create a Dataset object for data caching and batching\n",
    "    '''\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((features, onehot_labels))\n",
    "    \n",
    "     # Cache dataset\n",
    "    if cache_file:\n",
    "        dataset = dataset.cache(cache_file)\n",
    "\n",
    "    # Shuffle\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(len(features))\n",
    "    \n",
    "    # Repeat the dataset indefinitely\n",
    "    dataset = dataset.repeat()\n",
    "    \n",
    "    # Batch\n",
    "    dataset = dataset.batch(batch_size=batch_size)\n",
    "\n",
    "    # Prefetch\n",
    "    dataset = dataset.prefetch(buffer_size=1)\n",
    "    '''\n",
    "    \n",
    "    return features, new_labels\n",
    "\n",
    "def PiczakNet(input_shape):\n",
    "    \n",
    "    X_input = tf.keras.Input(input_shape)\n",
    "    \n",
    "    # First convolution block\n",
    "    model = tf.keras.layers.Conv2D(80, kernel_size=(57, 6), strides=1, padding='same', name='conv0')(X_input)\n",
    "    model = tf.keras.layers.Activation('relu')(model)\n",
    "    model = tf.keras.layers.MaxPool2D(pool_size=(4, 3), strides=(1, 3), padding='same')(model)\n",
    "    model = tf.keras.layers.Dropout(0.5)(model)\n",
    "    \n",
    "    # Second convolution block\n",
    "    model = tf.keras.layers.Conv2D(80, kernel_size=(1, 3), strides=1, padding='same', name='conv1')(model)\n",
    "    model = tf.keras.layers.Activation('relu')(model)\n",
    "    model = tf.keras.layers.MaxPool2D(pool_size=(1, 3), strides=(1, 3), padding='same')(model)\n",
    "    \n",
    "    # Flatten\n",
    "    model = tf.keras.layers.Flatten()(model)\n",
    "    \n",
    "    # First fully-connected block\n",
    "    model = tf.keras.layers.Dense(5000, activation='relu', name='fc0')(model)\n",
    "    model = tf.keras.layers.Dropout(0.5)(model)\n",
    "    \n",
    "    # Second fully-connected block\n",
    "    model = tf.keras.layers.Dense(5000, activation='relu', name='fc1')(model)\n",
    "    model = tf.keras.layers.Dropout(0.5)(model)\n",
    "    \n",
    "    # Output layer\n",
    "    model = tf.keras.layers.Dense(50, activation='softmax', name='out')(model)\n",
    "    \n",
    "    # Create model\n",
    "    model = tf.keras.Model(inputs = X_input, outputs = model, name='PiczakNet')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indie-status",
   "metadata": {},
   "source": [
    "## Load Data and Tet with RAW Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "extensive-detective",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:20<00:00, 24.96it/s]\n",
      "100%|██████████| 2000/2000 [00:00<00:00, 666767.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folds size: 400 - 400 - 400 - 400 - 400\n",
      "Folds sample shape:  2\n",
      "Folds sample data shape:  (110250,)\n",
      "Folds sample label type:  (50,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:02<00:00, 151.46it/s]\n",
      "100%|██████████| 400/400 [00:02<00:00, 150.21it/s]\n",
      "100%|██████████| 400/400 [00:02<00:00, 152.97it/s]\n",
      "100%|██████████| 400/400 [00:02<00:00, 151.98it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load raw data\n",
    "PATH = 'audio'\n",
    "raw_files, cvs, labels = Load_RAW(PATH)\n",
    "\n",
    "# Split the different folds\n",
    "f1, f2, f3, f4, f5 = Split_Folds(raw_files, cvs, labels, verbose=True)\n",
    "\n",
    "# split data from labels\n",
    "f1d, f1l = Split_Data_Label(f1)\n",
    "f2d, f2l = Split_Data_Label(f2)\n",
    "f3d, f3l = Split_Data_Label(f3)\n",
    "f4d, f4l = Split_Data_Label(f4)\n",
    "\n",
    "# process the data\n",
    "f1_processed, lf1_processed = Preprocessing(f1d, f1l)\n",
    "f2_processed, lf2_processed = Preprocessing(f2d, f2l)\n",
    "f3_processed, lf3_processed = Preprocessing(f3d, f3l)\n",
    "f4_processed, lf4_processed = Preprocessing(f4d, f4l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "short-exposure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60, 41, 2)\n"
     ]
    }
   ],
   "source": [
    "# Create training set\n",
    "merged_training_data = np.concatenate((f1_processed, f3_processed, f4_processed))\n",
    "merged_training_label = np.concatenate((lf1_processed, lf3_processed, lf4_processed))\n",
    "\n",
    "# Create and cache training\n",
    "training_dataset = tf.data.Dataset.from_tensor_slices((merged_training_data, merged_training_label))\n",
    "training_dataset = training_dataset.batch(batch_size=32)\n",
    "training_dataset = training_dataset.cache(\"training_cache\")\n",
    "training_dataset = training_dataset.prefetch(buffer_size=1)\n",
    "#training_dataset = training_dataset.repeat()\n",
    "\n",
    "# Create and cache validation\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices((f2_processed, lf2_processed))\n",
    "validation_dataset = validation_dataset.cache(\"validation_cache\")\n",
    "validation_dataset = validation_dataset.batch(batch_size=32)\n",
    "validation_dataset = validation_dataset.prefetch(buffer_size=1)\n",
    "#validation_dataset = validation_dataset.repeat()\n",
    "\n",
    "# Initialize the network\n",
    "input_shape = merged_training_data[0].shape\n",
    "print(input_shape)\n",
    "opt = tf.keras.optimizers.Adam(lr=0.0002)\n",
    "PiczakNet = PiczakNet(input_shape)\n",
    "PiczakNet.compile(optimizer=opt, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interior-clark",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 1\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "epoch_loss= []\n",
    "epoch_acc = []\n",
    "\n",
    "epoch_vl = []\n",
    "epoch_va = []\n",
    "\n",
    "# Loop over the epochs\n",
    "for epoch in range(max_epochs):\n",
    "    \n",
    "    # Shuffle the training\n",
    "    training_dataset = training_dataset.shuffle(len(merged_training_data))\n",
    "    \n",
    "    step_loss = []\n",
    "    step_acc = []\n",
    "    \n",
    "    step_vl = []\n",
    "    step_va = []\n",
    "    \n",
    "    # train over mini-batches\n",
    "    for x_batch, y_batch in training_dataset:\n",
    "        \n",
    "        # train on batch\n",
    "        step_stats = PiczakNet.train_on_batch(x_batch, y_batch)\n",
    "        \n",
    "        # save loss and accuracy\n",
    "        step_loss.append(step_stats[0])\n",
    "        step_acc.append(step_stats[1])\n",
    "        \n",
    "    # compute validation stats\n",
    "    for x_batch, y_batch in validation_dataset:\n",
    "        \n",
    "        # compute validation stats\n",
    "        val_stats = PiczakNet.test_on_batch(x_batch, y_batch)\n",
    "        \n",
    "        # save loss and accuracy\n",
    "        step_vl.append(val_stats[0])\n",
    "        step_va.append(val_stats[1])\n",
    "\n",
    "        \n",
    "    # Save the mean loss and accuracy of the entire epoch\n",
    "    epoch_loss.append(np.mean(step_loss))\n",
    "    epoch_acc.append(np.mean(step_acc))\n",
    "    epoch_vl.append(np.mean(step_vl))\n",
    "    epoch_va.append(np.mean(step_va))\n",
    "    \n",
    "    # Print epoch training stats\n",
    "    print(\"Epoch %2d: \\t t-loss: %3.6f \\t t-acc: %.6f \\t v-loss: %3.6f \\t v-acc: %.6f\" % (epoch + 1, epoch_loss[-1], epoch_acc[-1], epoch_vl[-1], epoch_va[-1]))\n",
    "    \n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aging-theory",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "underlying-laser",
   "metadata": {},
   "source": [
    "## Load and Test with Augmented Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "biblical-example",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:13<00:00, 152.74it/s]\n",
      "100%|██████████| 2000/2000 [00:12<00:00, 153.86it/s]\n",
      "100%|██████████| 2000/2000 [00:13<00:00, 150.90it/s]\n",
      "100%|██████████| 2000/2000 [00:13<00:00, 152.92it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load\n",
    "af1, alf1 = Load_Augmented(name='af1', path='Augmented_2/')\n",
    "af2, alf2 = Load_Augmented(name='af2', path='Augmented_2/')\n",
    "af3, alf3 = Load_Augmented(name='af3', path='Augmented_2/')\n",
    "af4, alf4 = Load_Augmented(name='af4', path='Augmented_2/')\n",
    "\n",
    "# Compute the features\n",
    "f1_processed, lf1_processed = Preprocessing(af1, alf1)\n",
    "f2_processed, lf2_processed = Preprocessing(af2, alf2)\n",
    "f3_processed, lf3_processed = Preprocessing(af3, alf3)\n",
    "f4_processed, lf4_processed = Preprocessing(af4, alf4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "square-science",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60, 41, 2)\n"
     ]
    }
   ],
   "source": [
    "# Create training set\n",
    "merged_training_data = np.concatenate((f1_processed, f3_processed, f4_processed))\n",
    "merged_training_label = np.concatenate((lf1_processed, lf3_processed, lf4_processed))\n",
    "\n",
    "# Create and cache training\n",
    "training_dataset = tf.data.Dataset.from_tensor_slices((merged_training_data, merged_training_label))\n",
    "training_dataset = training_dataset.batch(batch_size=32)\n",
    "training_dataset = training_dataset.cache(\"training_cache\")\n",
    "training_dataset = training_dataset.prefetch(buffer_size=1)\n",
    "#training_dataset = training_dataset.repeat()\n",
    "\n",
    "# Create and cache validation\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices((f2_processed, lf2_processed))\n",
    "validation_dataset = validation_dataset.cache(\"validation_cache\")\n",
    "validation_dataset = validation_dataset.batch(batch_size=32)\n",
    "validation_dataset = validation_dataset.prefetch(buffer_size=1)\n",
    "#validation_dataset = validation_dataset.repeat()\n",
    "\n",
    "# Initialize the network\n",
    "input_shape = f1_processed[0].shape\n",
    "print(input_shape)\n",
    "opt = tf.keras.optimizers.Adam(lr=0.0002)\n",
    "PiczakNet = PiczakNet(input_shape)\n",
    "PiczakNet.compile(optimizer=opt, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "honest-egyptian",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "iterat = iter(training_dataset)\n",
    "test = next(iterat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "heard-creativity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "563\n"
     ]
    }
   ],
   "source": [
    "print(len(training_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "equipped-europe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1: \t t-loss: 3.277637 \t t-acc: 0.119539 \t v-loss: 3.532025 \t v-acc: 0.112955 \t time: 97.125\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 1\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "epoch_loss= []\n",
    "epoch_acc = []\n",
    "\n",
    "epoch_vl = []\n",
    "epoch_va = []\n",
    "\n",
    "# Loop over the epochs\n",
    "for epoch in range(max_epochs):\n",
    "    \n",
    "    # Shuffle the training\n",
    "    training_dataset = training_dataset.shuffle(len(merged_training_data))\n",
    "    \n",
    "    step_loss = []\n",
    "    step_acc = []\n",
    "    \n",
    "    step_vl = []\n",
    "    step_va = []\n",
    "    \n",
    "    start = time.time()\n",
    "    # train over mini-batches\n",
    "    for x_batch, y_batch in training_dataset:\n",
    "        \n",
    "        # train on batch\n",
    "        step_stats = PiczakNet.train_on_batch(x_batch, y_batch)\n",
    "        \n",
    "        # save loss and accuracy\n",
    "        step_loss.append(step_stats[0])\n",
    "        step_acc.append(step_stats[1])\n",
    "        \n",
    "    # compute validation stats\n",
    "    for x_batch, y_batch in validation_dataset:\n",
    "        \n",
    "        # compute validation stats\n",
    "        val_stats = PiczakNet.test_on_batch(x_batch, y_batch)\n",
    "        \n",
    "        # save loss and accuracy\n",
    "        step_vl.append(val_stats[0])\n",
    "        step_va.append(val_stats[1])\n",
    "    end = time.time()\n",
    "        \n",
    "    # Save the mean loss and accuracy of the entire epoch\n",
    "    epoch_loss.append(np.mean(step_loss))\n",
    "    epoch_acc.append(np.mean(step_acc))\n",
    "    epoch_vl.append(np.mean(step_vl))\n",
    "    epoch_va.append(np.mean(step_va))\n",
    "    \n",
    "    # Print epoch training stats\n",
    "    print(\"Epoch %2d: \\t t-loss: %3.6f \\t t-acc: %.6f \\t v-loss: %3.6f \\t v-acc: %.6f \\t time: %3.3f\" % (epoch + 1, epoch_loss[-1], epoch_acc[-1], epoch_vl[-1], epoch_va[-1], (end - start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caroline-network",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
