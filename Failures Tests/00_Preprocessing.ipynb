{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "statewide-centre",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 5735551442313802692\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 12927546576105909665\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 7033930304\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 9501453520547569837\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 7809477673525260814\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import h5py\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display\n",
    "import librosa.display\n",
    "\n",
    "# Use GPU\n",
    "from tensorflow.python.client import device_lib \n",
    "print(device_lib.list_local_devices())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "static-label",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "therapeutic-venue",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw data\n",
    "def Load_RAW(path):\n",
    "    '''\n",
    "        Input:\n",
    "            path: folder of the dataset\n",
    "        \n",
    "        Output:\n",
    "            raw_data:  list that contains the raw data\n",
    "            cvs:       list that contains the cross-fold number\n",
    "            labels:    list that contains the category information\n",
    "    '''\n",
    "    \n",
    "    # Container for the dataset\n",
    "    raw_data = []\n",
    "    cvs = []\n",
    "    labels = []\n",
    "    # Load every file inside the folder\n",
    "    for file_name in tqdm(os.listdir(path)):\n",
    "\n",
    "        try:\n",
    "            # Get audio data and sampling rate\n",
    "            audio, sampling_rate = librosa.load(os.path.join(path, file_name), res_type='kaiser_fast')\n",
    "            # Split the file name\n",
    "            name_splitted = re.split('[-.]', file_name)\n",
    "            \n",
    "            # Append a row of 3 elements\n",
    "            raw_data.append(audio)\n",
    "            cvs.append(name_splitted[0])\n",
    "            labels.append(name_splitted[3])\n",
    "        except Exception as e:\n",
    "            pass\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    raw_audio = np.asarray(raw_data)\n",
    "    cvs = np.asarray(cvs, dtype=int)\n",
    "    labels = np.asarray(labels, dtype=int)\n",
    "    \n",
    "    # onehot encode the labels in 50 classes\n",
    "    onehot_labels = to_categorical(labels, num_classes=50)\n",
    "    \n",
    "    return raw_audio, cvs, onehot_labels\n",
    "\n",
    "\n",
    "\n",
    "def Split_Folds(raw_audio, cvs, labels, verbose=False):\n",
    "    '''\n",
    "        Input:\n",
    "            raw_audio: list that contains the raw data\n",
    "            cvs:       list that contains the cross-fold number\n",
    "            labels:    list that contains the category information\n",
    "            verbose:   flag used to print produced folds information\n",
    "        \n",
    "        Output:\n",
    "            f{1,2,3,4,5}:      folds that contains the raw data and labels\n",
    "    '''\n",
    "    \n",
    "    f1 = []\n",
    "    f2 = []\n",
    "    f3 = []\n",
    "    f4 = []\n",
    "    f5 = []\n",
    "    \n",
    "    # Loop over each file audio\n",
    "    for num, audio in enumerate(tqdm(raw_audio)):\n",
    "        \n",
    "        if cvs[num] == 1:\n",
    "            f1.append((audio, labels[num]))\n",
    "        elif cvs[num] == 2:\n",
    "            f2.append([audio, labels[num]])\n",
    "        elif cvs[num] == 3:\n",
    "            f3.append([audio, labels[num]])\n",
    "        elif cvs[num] == 4:\n",
    "            f4.append([audio, labels[num]])\n",
    "        elif cvs[num] == 5:\n",
    "            f5.append([audio, labels[num]])\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    f1 = np.asarray(f1, dtype=object)\n",
    "    f2 = np.asarray(f2, dtype=object)\n",
    "    f3 = np.asarray(f3, dtype=object)\n",
    "    f4 = np.asarray(f4, dtype=object)\n",
    "    f5 = np.asarray(f5, dtype=object)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Folds size: %2d - %2d - %2d - %2d - %2d\" % (len(f1), len(f2), len(f3), len(f4), len(f5)))\n",
    "\n",
    "        print(\"Folds sample shape: \", len(f1[0]))\n",
    "\n",
    "        print(\"Folds sample data shape: \", f1[0][0].shape)\n",
    "        \n",
    "        print(\"Folds sample label type: \", f1[0][1].shape)\n",
    "    \n",
    "    return f1, f2, f3, f4, f5\n",
    "\n",
    "\n",
    "# Generate noise augmentation\n",
    "def Noise_Augmentation(data, number):\n",
    "    '''\n",
    "        Input:\n",
    "            data:              data sample to augment\n",
    "            number:            number of augmentations\n",
    "        \n",
    "        Output:\n",
    "            augmented_data:    augmented data with noise\n",
    "    '''\n",
    "    noise_factor = np.random.uniform(0.001, 0.05, number)\n",
    "    noise = np.random.randn(number, len(data))\n",
    "    \n",
    "    tmp = []\n",
    "    for n, sample in enumerate(noise):\n",
    "        tmp.append(noise_factor[n] * sample)\n",
    "    \n",
    "    augmented_data = data + tmp\n",
    "    \n",
    "    # Cast back to same data type\n",
    "    augmented_data = augmented_data.astype(type(data[0]))\n",
    "    augmented_data = np.asarray(augmented_data)\n",
    "    return augmented_data\n",
    "\n",
    "\n",
    "# Generate shifting augmentation\n",
    "def Pitch_Augmentation(data, number):\n",
    "    '''\n",
    "        Input:\n",
    "            data:              data sample to augment\n",
    "            number:            number of augmentations\n",
    "        \n",
    "        Output:\n",
    "            augmented_data:    augmented data with pitch variation\n",
    "    '''\n",
    "    pitch_factor = np.random.uniform(-12, 12, number)\n",
    "    pitched = []\n",
    "    \n",
    "    for i in range(number):\n",
    "        pitched.append(librosa.effects.pitch_shift(data, 22050, pitch_factor[i]))\n",
    "    \n",
    "    pitched = np.asarray(pitched)\n",
    "    return pitched\n",
    "\n",
    "\n",
    "# Speed Augmentation \n",
    "############################################################## RETURN VARIABLE SIZE DATA, NEED TO FIX IN SOMEHOW TO BE USABLE #################\n",
    "def Speed_Augmentation(data, number):\n",
    "    speed_factor = np.random.uniform(1, 12, number)\n",
    "    #print(speed_factor)\n",
    "    speed = []\n",
    " \n",
    "    for i in range(number):\n",
    "        speed.append(np.asarray(librosa.effects.time_stretch(data, speed_factor[i])))\n",
    "    \n",
    "    speed = np.asarray(speed).reshape(num, len(speed[0]))\n",
    "    return speed\n",
    "\n",
    "\n",
    "def Merge_Data_Label(data, label):\n",
    "    \n",
    "    \n",
    "    new_dataset = []\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        new_dataset.append([data[i], label[i]])\n",
    "    \n",
    "    new_dataset = np.asarray(new_dataset, dtype=object)\n",
    "    \n",
    "    return new_dataset\n",
    "\n",
    "\n",
    "def Split_Data_Label(dataset):\n",
    "    \n",
    "    \n",
    "    data = []\n",
    "    label = []\n",
    "    \n",
    "    for i in range (len(dataset)):\n",
    "        data.append(dataset[i][0])\n",
    "        label.append(dataset[i][1])\n",
    "\n",
    "    \n",
    "    data = np.asarray(data)\n",
    "    label = np.asarray(label)\n",
    "    \n",
    "    return data, label\n",
    "\n",
    "\n",
    "\n",
    "# Perform data augmentation on the original dataset\n",
    "def Data_Augmentation(dataset, number, name='', path='Augmented_Data/', save=False, noise=False, pitch=False):\n",
    "    '''\n",
    "        Input:\n",
    "            dataset:           dataset to augment\n",
    "            number:            number of augmentations for each sample per type\n",
    "            name:              name of the file if save\n",
    "            path:              path in which save the data\n",
    "            save:              flag for saving augmented data\n",
    "            noise:             flag to enable noise augmentation\n",
    "            pitch:             flag to enable pitching augmentation\n",
    "            \n",
    "        \n",
    "        Output:\n",
    "            new_dataset:       augmented data\n",
    "    '''\n",
    "    new_dataset = []\n",
    "    \n",
    "    # Loop over each sample and augment according to the input flags\n",
    "    for sample in tqdm(dataset):\n",
    "        \n",
    "        # Append the original sample\n",
    "        new_dataset.append([sample[0], sample[1]])\n",
    "        \n",
    "        # Generate noisy samples\n",
    "        if(noise):\n",
    "            noise_samples = Noise_Augmentation(sample[0], number)\n",
    "            \n",
    "            # Append the generated samples\n",
    "            for gen in noise_samples:\n",
    "                new_dataset.append([gen, sample[1]])\n",
    "        \n",
    "        # Generate pitched samples\n",
    "        if(pitch):\n",
    "            pitch_samples = Pitch_Augmentation(sample[0], number)\n",
    "            \n",
    "            # Append the generated samples\n",
    "            for gen in pitch_samples:\n",
    "                new_dataset.append([gen, sample[1]])\n",
    "    \n",
    "    new_dataset = np.asarray(new_dataset, dtype=object)\n",
    "    \n",
    "    # Get splitted version\n",
    "    d2s, l2s = Split_Data_Label(new_dataset)\n",
    "    \n",
    "    d2s = np.asarray(d2s, dtype=np.float32)\n",
    "    l2s = np.asarray(l2s, dtype=np.float32)\n",
    "    \n",
    "    if(save):\n",
    "        hf = h5py.File(path + name + '.h5', 'w')\n",
    "        hf.create_dataset('data', data=d2s)\n",
    "        hf.create_dataset('label', data=l2s)\n",
    "        hf.close()\n",
    "        \n",
    "        \n",
    "    return new_dataset\n",
    "\n",
    "\n",
    "# Load saved data\n",
    "def Load_Augmented(name='', path='Augmented_Data/'):\n",
    "    '''\n",
    "        Input:\n",
    "            name:      name of the file\n",
    "            path:      path of the file\n",
    "        \n",
    "        Output:\n",
    "            dataset:   loaded dataset with data and labels\n",
    "    '''\n",
    "    hf = h5py.File(path + name + '.h5', 'r')\n",
    "    data =  np.array(hf.get('data'))\n",
    "    labels = np.array(hf.get('label'))\n",
    "    hf.close()\n",
    "    \n",
    "    data = np.asarray(data)\n",
    "    label = np.asarray(labels)\n",
    "    '''\n",
    "    dataset = []\n",
    "    for i in range(len(data)):\n",
    "        dataset.append([data[i], labels[i]])\n",
    "    \n",
    "    dataset = np.asarray(dataset, dtype=object)\n",
    "    '''\n",
    "    return data, labels\n",
    "\n",
    "\n",
    "# Preprocessing\n",
    "############################# PARAMETERS FOR DATA LOADING CACHING BATCHING:  batch_size, shuffle, cache_file=None,\n",
    "def Preprocessing(raw_audio, labels, bands=60, frames=41):\n",
    "    '''\n",
    "        Input:\n",
    "            raw_audio:     list that contains the raw/augmented data\n",
    "            labels:        list that contains the category information\n",
    "            bands:         number of mel band to use\n",
    "            frames:        number of frames to use\n",
    "        \n",
    "        Output:\n",
    "            features:      numpy array that contains processed audio data with log-melspec and delta\n",
    "            new_labels:    new labels for each augmented segment\n",
    "    '''    \n",
    "    \n",
    "    new_labels = []\n",
    "    augmented_spec = []\n",
    "    \n",
    "    # Normalize the raw data\n",
    "    norm_factor = np.percentile(raw_audio, 99) - np.percentile(raw_audio, 5)\n",
    "    raw_audio = raw_audio / norm_factor\n",
    "    \n",
    "    # Loop over each file audio\n",
    "    for num, audio in enumerate(tqdm(raw_audio)):\n",
    "    \n",
    "        # Convert audio to melspectogram\n",
    "        '''\n",
    "            With default n_fft=2048 we have the filter size of 2048/2+1=1025 [Nyquist Frequency]\n",
    "        '''\n",
    "        melspec = librosa.feature.melspectrogram(audio, n_mels=bands, hop_length=512)\n",
    "        \n",
    "        # Convert melspec to log melspec\n",
    "        logspec = librosa.core.amplitude_to_db(melspec)\n",
    "        \n",
    "        counter = 0\n",
    "        # Spectrogram splitting with 50% overlap and adapt cv-fold and labels info\n",
    "        for idx in range(0, len(logspec[0]) - frames, int(frames/2)):\n",
    "            augmented_spec.append(logspec[:, idx:idx+frames])\n",
    "            new_labels.append(labels[num])\n",
    "            counter = counter +1\n",
    "            \n",
    "    # Reshape the outputs\n",
    "    log_specgrams = np.asarray(augmented_spec).reshape(len(augmented_spec), bands, frames, 1)\n",
    "    features = np.concatenate((log_specgrams, np.zeros(np.shape(log_specgrams))), axis = 3)\n",
    "    new_labels = np.asarray(new_labels, dtype=int)\n",
    "    \n",
    "    # Fill the delta features\n",
    "    for i in range(len(log_specgrams)):\n",
    "        features[i, :, :, 1] = librosa.feature.delta(features[i, :, :, 0])\n",
    "    \n",
    "    ######  Create a Dataset object for data caching and batching\n",
    "    '''\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((features, onehot_labels))\n",
    "    \n",
    "     # Cache dataset\n",
    "    if cache_file:\n",
    "        dataset = dataset.cache(cache_file)\n",
    "\n",
    "    # Shuffle\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(len(features))\n",
    "    \n",
    "    # Repeat the dataset indefinitely\n",
    "    dataset = dataset.repeat()\n",
    "    \n",
    "    # Batch\n",
    "    dataset = dataset.batch(batch_size=batch_size)\n",
    "\n",
    "    # Prefetch\n",
    "    dataset = dataset.prefetch(buffer_size=1)\n",
    "    '''\n",
    "    \n",
    "    return features, new_labels\n",
    "\n",
    "# Preprocessing\n",
    "def Filtered_Preprocessing(raw_audio, labels, bands=60, frames=41):\n",
    "    '''\n",
    "        Input:\n",
    "            raw_audio:     list that contains the raw/augmented data\n",
    "            labels:        list that contains the category information\n",
    "            bands:         number of mel band to use\n",
    "            frames:        number of frames to use\n",
    "        \n",
    "        Output:\n",
    "            features:      numpy array that contains processed audio data with log-melspec and delta\n",
    "            new_labels:    new labels for each augmented segment\n",
    "    '''    \n",
    "\n",
    "    \n",
    "    segments = []\n",
    "    segment_labels = []\n",
    "    \n",
    "    augmented_spec = []\n",
    "    new_labels = []\n",
    "    \n",
    "    # Normalize the raw data\n",
    "    norm_factor = np.percentile(raw_audio, 99) - np.percentile(raw_audio, 5)\n",
    "    raw_audio = raw_audio / norm_factor\n",
    "    \n",
    "    # Loop over each file audio and divide into segments\n",
    "    for num, audio in enumerate(tqdm(raw_audio)):\n",
    "    \n",
    "        # Convert audio to melspectogram\n",
    "        '''\n",
    "            With default n_fft=2048 we have the filter size of 2048/2+1=1025 [Nyquist Frequency]\n",
    "        '''\n",
    "        melspec = librosa.feature.melspectrogram(audio, n_mels=bands, hop_length=512)\n",
    "\n",
    "        # Spectrogram splitting with 50% overlap and adapt cv-fold and labels info\n",
    "        for idx in range(0, len(melspec[0]) - frames, int(frames/2)):\n",
    "            segments.append(melspec[:, idx:idx+frames])\n",
    "            segment_labels.append(labels[num])\n",
    "        \n",
    "    # Check and ignore silent segments\n",
    "    for i, segment in enumerate(tqdm(segments)):\n",
    "        \n",
    "        #S = librosa.feature.inverse.mel_to_stft(segment)\n",
    "        #segment_audio = librosa.griffinlim(S)\n",
    "        \n",
    "        # Append only non silent segments and convert into db\n",
    "        if(np.mean(segment) >= 0.0001):\n",
    "            augmented_spec.append(segment)\n",
    "            new_labels.append(segment_labels[i])\n",
    "    \n",
    "    augmented_spec = np.asarray(augmented_spec)\n",
    "    logspec = librosa.core.amplitude_to_db(augmented_spec)\n",
    "    \n",
    "    # Reshape the outputs\n",
    "    log_specgrams = np.asarray(logspec).reshape(len(augmented_spec), bands, frames, 1)\n",
    "    features = np.concatenate((log_specgrams, np.zeros(np.shape(log_specgrams))), axis = 3)\n",
    "    new_labels = np.asarray(new_labels, dtype=int)\n",
    "    \n",
    "    # Fill the delta features\n",
    "    for i in range(len(log_specgrams)):\n",
    "        features[i, :, :, 1] = librosa.feature.delta(features[i, :, :, 0])\n",
    "    \n",
    "    return features, new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "basic-truth",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:23<00:00, 24.06it/s]\n"
     ]
    }
   ],
   "source": [
    "# Load raw data\n",
    "PATH = 'audio'\n",
    "raw_files, cvs, labels = Load_RAW(PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "excess-maryland",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:00<00:00, 666874.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folds size: 400 - 400 - 400 - 400 - 400\n",
      "Folds sample shape:  2\n",
      "Folds sample data shape:  (110250,)\n",
      "Folds sample label type:  (50,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Split the different folds\n",
    "f1, f2, f3, f4, f5 = Split_Folds(raw_files, cvs, labels, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "correct-heading",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:02<00:00, 153.91it/s]\n",
      "100%|██████████| 3600/3600 [00:00<00:00, 60001.96it/s]\n"
     ]
    }
   ],
   "source": [
    "# Process without augmentation\n",
    "f1_data, f1_label = Split_Data_Label(f1)\n",
    "f1_processed, lf1_processed = Filtered_Preprocessing(f1_data, f1_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "grave-integral",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3313, 60, 41, 2)\n"
     ]
    }
   ],
   "source": [
    "print(f1_processed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joint-memorial",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augment and save fold-1\n",
    "augmented_f1 = Data_Augmentation(f1, 2, name='af1', path='Augmented_3/', save=True, noise=True, pitch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wrapped-heading",
   "metadata": {},
   "outputs": [],
   "source": [
    "#process augmented\n",
    "af1_data, af1_label = Split_Data_Label(augmented_f1)\n",
    "f1_processed, lf1_processed = Preprocessing(af1_data, af1_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disturbed-corrections",
   "metadata": {},
   "source": [
    "## OLD STUFFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "julian-cliff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augment and save fold-1\n",
    "augmented_f1 = Data_Augmentation(f1, 2, name='af1', path='Augmented_2/', save=True, noise=True, pitch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "global-quality",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augment and save fold-2\n",
    "augmented_f2 = Data_Augmentation(f2, 2, name='af2', path='Augmented_2/', save=True, noise=True, pitch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "circular-coupon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augment and save fold-3\n",
    "augmented_f3 = Data_Augmentation(f3, 2, name='af3', path='Augmented_2/', save=True, noise=True, pitch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scenic-mexico",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augment and save fold-4\n",
    "augmented_f4 = Data_Augmentation(f4, 2, name='af4', path='Augmented_2/', save=True, noise=True, pitch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "choice-village",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augment and save fold-5\n",
    "augmented_f5 = Data_Augmentation(f5, 2, name='af5', path='Augmented_2/', save=True, noise=True, pitch=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enclosed-torture",
   "metadata": {},
   "source": [
    "## Load Saved Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "magnetic-galaxy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Augmented Data\n",
    "af1, alf1 = Load_Augmented(name='af1', path='Augmented_Data/')\n",
    "af2, alf2 = Load_Augmented(name='af2', path='Augmented_Data/')\n",
    "#af3, alf3 = Load_Augmented(name='af3', path='Augmented_Data/')\n",
    "#af4, alf4 = Load_Augmented(name='af4', path='Augmented_Data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lyric-buddy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the features\n",
    "f1_processed, lf1_processed = Preprocessing(af1, alf1)\n",
    "f2_processed, lf2_processed = Preprocessing(af2, alf2)\n",
    "#f3_processed, lf3_processed = Preprocessing(af3, alf3)\n",
    "#f4_processed, lf4_processed = Preprocessing(af4, alf4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "small-barrel",
   "metadata": {},
   "source": [
    "## Generate Trainind Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powered-genius",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the 3 folds\n",
    "#merged_data = np.concatenate((f1_processed, f2_processed, f3_processed))\n",
    "#merged_label = np.concatenate((lf1_processed, lf2_processed, lf3_processed))\n",
    "\n",
    "\n",
    "# Generate dataset for merged data\n",
    "dataset = tf.data.Dataset.from_tensor_slices((f1_processed, lf1_processed))\n",
    "\n",
    "# Cache the data\n",
    "dataset = dataset.cache(\"training_cache\")\n",
    "\n",
    "# Shuffle the data\n",
    "dataset = dataset.shuffle(len(f1_processed))\n",
    "\n",
    "batch_size = 32\n",
    "'''\n",
    " # Cache dataset\n",
    "if cache_file:\n",
    "    dataset = dataset.cache(cache_file)\n",
    "\n",
    "# Shuffle\n",
    "if shuffle:\n",
    "    dataset = dataset.shuffle(len(features))\n",
    "'''\n",
    "# Repeat the dataset indefinitely\n",
    "dataset = dataset.repeat()\n",
    "\n",
    "# Batch\n",
    "dataset = dataset.batch(batch_size=batch_size)\n",
    "\n",
    "# Prefetch\n",
    "dataset = dataset.prefetch(buffer_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funky-gross",
   "metadata": {},
   "source": [
    "## Generate Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beautiful-value",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataset for merged data\n",
    "validation = tf.data.Dataset.from_tensor_slices((f2_processed, lf2_processed))\n",
    "\n",
    "# Cache the data\n",
    "validation_dataset = validation.cache(\"validation_cache\")\n",
    "\n",
    "\n",
    "# Repeat the dataset indefinitely\n",
    "validation_dataset = dataset.repeat()\n",
    "\n",
    "# Batch\n",
    "validation_dataset = dataset.batch(batch_size=batch_size)\n",
    "\n",
    "# Prefetch\n",
    "validation_dataset = dataset.prefetch(buffer_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "auburn-coast",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PiczakNet(input_shape):\n",
    "    \n",
    "    X_input = tf.keras.Input(input_shape)\n",
    "    \n",
    "    # First convolution block\n",
    "    model = tf.keras.layers.Conv2D(80, kernel_size=(57, 6), strides=1, padding='same', name='conv0')(X_input)\n",
    "    model = tf.keras.layers.Activation('relu')(model)\n",
    "    model = tf.keras.layers.MaxPool2D(pool_size=(4, 3), strides=(1, 3), padding='same')(model)\n",
    "    model = tf.keras.layers.Dropout(0.5)(model)\n",
    "    \n",
    "    # Second convolution block\n",
    "    model = tf.keras.layers.Conv2D(80, kernel_size=(1, 3), strides=1, padding='same', name='conv1')(model)\n",
    "    model = tf.keras.layers.Activation('relu')(model)\n",
    "    model = tf.keras.layers.MaxPool2D(pool_size=(1, 3), strides=(1, 3), padding='same')(model)\n",
    "    \n",
    "    # Flatten\n",
    "    model = tf.keras.layers.Flatten()(model)\n",
    "    \n",
    "    # First fully-connected block\n",
    "    model = tf.keras.layers.Dense(5000, activation='relu', name='fc0')(model)\n",
    "    model = tf.keras.layers.Dropout(0.5)(model)\n",
    "    \n",
    "    # Second fully-connected block\n",
    "    model = tf.keras.layers.Dense(5000, activation='relu', name='fc1')(model)\n",
    "    model = tf.keras.layers.Dropout(0.5)(model)\n",
    "    \n",
    "    # Output layer\n",
    "    model = tf.keras.layers.Dense(50, activation='softmax', name='out')(model)\n",
    "    \n",
    "    # Create model\n",
    "    model = tf.keras.Model(inputs = X_input, outputs = model, name='PiczakNet')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaptive-track",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_shape = f1_processed[0].shape\n",
    "#opt = tf.keras.optimizers.Adam(lr=0.0002)\n",
    "\n",
    "PiczakNet = PiczakNet([60, 41, 2])\n",
    "#PiczakNet.compile(optimizer=opt, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "PiczakNet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documented-switch",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "train_steps = int(np.ceil(len(f1_processed)/batch_size))\n",
    "val_steps = int(np.ceil(len(validation)/batch_size))\n",
    "\n",
    "history = PiczakNet.fit(dataset,\n",
    "                        epochs=num_epochs, \n",
    "                        validation_data=validation_dataset, \n",
    "                        validation_steps=val_steps,\n",
    "                        steps_per_epoch=train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offensive-lodge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot loss\n",
    "plt.figure()\n",
    "plt.plot(history.history['loss'], label='Train loss')\n",
    "plt.plot(history.history['val_loss'], label='Val loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "\n",
    "# Plot accuracy\n",
    "plt.figure()\n",
    "plt.plot(history.history['accuracy'], label='Train accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Val accuracy')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constitutional-casting",
   "metadata": {},
   "source": [
    "## Save The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "residential-seeker",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = f'Augmented_Data/one_fold_model.h5'\n",
    "PiczakNet.save(filename)\n",
    "\n",
    "###### Load net\n",
    "#PiczakNet= load_model('Augmented_Data/one_fold_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "devoted-module",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(PiczakNet.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charitable-brook",
   "metadata": {},
   "source": [
    "## Test on Other Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hired-captain",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process fold 3\n",
    "af3, alf3 = Load_Augmented(name='af3', path='Augmented_Data/')\n",
    "f3_processed, lf3_processed = Preprocessing(af3, alf3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "potential-scratch",
   "metadata": {},
   "outputs": [],
   "source": [
    "PiczakNet.evaluate(f3_processed, lf3_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspended-library",
   "metadata": {},
   "outputs": [],
   "source": [
    "f4d, f4l = Split_Data_Label(f4)\n",
    "f4_processed, lf4_processed = Preprocessing(f4d, f4l)\n",
    "PiczakNet.evaluate(f4_processed, lf4_processed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decimal-rental",
   "metadata": {},
   "outputs": [],
   "source": [
    "PiczakNet.evaluate(f4_processed, lf4_processed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inclusive-madonna",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1d, f1l = Split_Data_Label(f1)\n",
    "f1_processed2, lf1_processed2 = Preprocessing(f1d, f1l)\n",
    "PiczakNet.evaluate(f1_processed2, lf1_processed2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worst-potato",
   "metadata": {},
   "outputs": [],
   "source": [
    "f2d, f2l = Split_Data_Label(f2)\n",
    "f2_processed2, lf2_processed2 = Preprocessing(f2d, f2l)\n",
    "PiczakNet.evaluate(f2_processed2, lf2_processed2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "owned-brooklyn",
   "metadata": {},
   "outputs": [],
   "source": [
    "f3d, f3l = Split_Data_Label(f3)\n",
    "f3_processed, lf3_processed = Preprocessing(f3d, f3l)\n",
    "PiczakNet.evaluate(f3_processed, lf3_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swiss-istanbul",
   "metadata": {},
   "outputs": [],
   "source": [
    "f5d, f5l = Split_Data_Label(f5)\n",
    "f5_processed, lf5_processed = Preprocessing(f5d, f5l)\n",
    "PiczakNet.evaluate(f5_processed, lf5_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brutal-playback",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
